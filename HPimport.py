# -*- coding: utf-8 -*-
"""
Created on Wed Jun 19 11:14:24 2024

@author: LENLUI
"""

import os
from os.path import join as cmb
import numpy as np
import pandas as pd
from io import StringIO
from datetime import datetime, timedelta
import time
import pytz
import struct
import headerMappingsHP as hHP
from createOutput import save_dataframe_with_dates, convert_excel_output, create_output_dataframe
from enthalpyCalculations import process_minute_data, process_hour_data
import pickle
import shutil
import sys
import fitz  # PyMuPDF
import re

sys.path.append('Automatic excel calculations/')
sys.path.append('knmi/')
from automatic_excel_proccssing import create_word_documents
from knmi import get_hour_data_dataframe
from knmibodemtemp import get_soil_temp_full

# Specify global variables
bReadData = True
bWriteExcel = True
bDebugStopExecutionHere = False # This is used before to run seperate file for the stress test of the script in between
bReadPickles = False # This is used to read the pickles from the previous run, good for debugging, but not for normal use. It will than use var_names to read the pickles
bRunPreviousWeek = True # This variable needs to be True if the script is runned automatically, to get the previous week data 
bReportWord = True # This is used for reporting. For automated reporting, set to True.
KNMI_SOIL_CORRECTION = 0.65

# If bReadPickles is True, the following variables will be read from the pickles
# var_names = ['change_files', 'df_1hr_newheaders', 'sMeetsetFolder', 'df_1min_newheaders', 'outliers_count'] # Used to save and read variables from pickles
var_names = ['df_1hr_newheaders', 'sMeetsetFolder', 'df_1min_newheaders', 'outliers_count'] # Used to save and read variables from pickles

# By default, the script will use the first two arguments as the meetset folder and location
# One can use this by running the script from the command line with the meetset folder and location as arguments. For example python HPimport.py "Meetset1-Deventer" "Deventer"
if len(sys.argv) > 2:
    sMeetsetFolder = sys.argv[1]
    location = sys.argv[2]
else:
    # Else use the default values, that can be set here manually:
    # sMeetsetFolder = 'StresstestDeventer_08-12jul'
    #sMeetsetFolder = 'Meetset3-Wijhe'
    sMeetsetFolder = 'Meetset2-Nunspeet'
    #sMeetsetFolder = 'Meetset1-Deventer'
    location = 'Nunspeet' # Used for Automatic excel calculations within Word documents

# Set the knmi station closest to the location
if location == 'Deventer':
    knmi_station = 275 # Station Deelen, 25 km from Deventer
elif location == 'Nunspeet':
    knmi_station = 260 # Station de Bilt, 45 km from Nunspeet
elif location == 'Wijhe':
    knmi_station = 278 # Station Heino, 10 km from Wijhe 
else:
    raise ValueError(f"Unknown location '{location}'. Please set knmi_station accordingly.")

if bRunPreviousWeek:
    time_now = datetime.now()
    weekno = time_now.isocalendar()[1]
    firstweekday = time_now - timedelta(days=time_now.weekday() + 8) # Go back 8 days to the previous week
    lastweekday = firstweekday + timedelta(days=7) # Go forward 7 days to the Sunday
    sDateStart = firstweekday.strftime('%Y-%m-%d')
    sDateEnd = lastweekday.strftime('%Y-%m-%d')
else:
    # Below is the option to set the date range manually

    # sDateStart = '2024-11-24'
    # sDateEnd = '2026-11-25'

    # sDateStart = '2022-11-24'
    # sDateEnd = '2024-11-25'

    sDateStart = '2025-06-22'
    sDateEnd = '2026-05-19'

# Some global constants
minimum_WP_power = 500 # Minimum power in We to consider the WP to be running
spec_heat_water = 4.19  # kJ/kgK
# Some coefficients, that were generated by Bertus for a fitted COP curve:
iii = 8.485073
iis = 0.13324
sii = -3.32107
sis = -0.06643
isi = -0.12466
iss = -0.00335
ssi = 0.062575
sss = 0.002186
max_power_wm1 = 12000 # Max power in warmtepomp 1, in We
max_power_wm2 = 17000 # Max power in warmtepomp 2, in We
maximum_interpolate_nans = 20 # Number of NaNs to interpolate in a row before setting the rest to nan
atmospheric_pressure = 1.023  # Atmospheric pressure in bar
minimum_atmospheric_pressure = 700 # Minimum pressure in mbar, below which the pressure is considered to be invalid and atmospheric_pressure is used
# For all variables there is a min and max values setted in minmax_cols.xlsx, but TgasIn and PgasIn are also set here:
TgasIn_min = -5.0
TgasIn_max = 100.0
PgasIn_min = -2.0
PgasIn_max = 70.0

if (bDebugStopExecutionHere) and (not location=='Deventer'):
    raise Exception("Debug stop execution here, because bDebugStopExecutionHere is True and location is not Deventer. This is used to run the script in a separate file for stress testing for Deventer with some specific changes.")


def save_variables(variables, folder='saveTemporaryPickles'):
    if not os.path.exists(folder):
        os.makedirs(folder)
    
    for var_name, var_value in variables.items():
        with open(os.path.join(folder, f'{var_name}.pkl'), 'wb') as f:
            pickle.dump(var_value, f)

def read_variables(var_names, folder='saveTemporaryPickles'):
    variables = {}
    for var_name in var_names:
        with open(os.path.join(folder, f'{var_name}.pkl'), 'rb') as f:
            variables[var_name] = pickle.load(f)
    return variables

def process_and_save(pBase, pInput, pPickles, sMeetset):
    pMeetset = cmb(pInput, sMeetset)
    output_subdir = cmb(pPickles, sMeetset)
    os.makedirs(output_subdir, exist_ok=True)
    print(f'Processing and saving pickles for set {sMeetset}...')

    # Compare ingested file dates in the form of '240607' with lstDone, skip already ingested files.
    lstDone = sorted([f.split('_')[1][2:10].replace('-','') for f in os.listdir(output_subdir) 
                        if (f.endswith('.bz2') and not '_partial' in f)])
    all_files = sorted([f for f in os.listdir(pMeetset) if (f.endswith('.txt') and 
                    not f.split('_')[-2] in lstDone and not '-test' in f and not '-config' in f)])
    daily_data = []
    current_day = None
    lstTimes = []

    for file in all_files:
        print(f' - Reading file {file}..')
        sData, header_df = read_file(cmb(pMeetset, file))
        dfEmpty = create_multi_index_df(header_df)
        tStart = time.time()
        data = process_datafile(sData, dfEmpty, header_df)
        lstTimes.append(time.time()-tStart)

        if not data.empty:
            data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], format='%y/%m/%d %H:%M:%S')
            file_date = data['Timestamp'].iloc[0].iloc[0].date()  # Assuming 'timestamp' column exists and is datetime type
#EDIT python 3.12: iloc[0][0] no longer valid
            if current_day is None:
                current_day = file_date
            if file_date != current_day:
                # Save the accumulated data for the previous day
                daily_df = pd.concat(daily_data)
                daily_df.reset_index(inplace=True, drop=True)
                daily_date = daily_df['Timestamp'].iloc[0].iloc[0].date()
#EDIT python 3.12: iloc[0][0] no longer valid                
                print(f'Saving daily data for date {daily_date}..')
                daily_df = group_and_combine_columns(daily_df)
                daily_df.to_pickle(cmb(output_subdir, f'{sMeetset}_{daily_date}.bz2'))
                if os.path.exists(cmb(output_subdir, f'{sMeetset}_{daily_date}_partial.bz2')):
                    os.remove(cmb(output_subdir, f'{sMeetset}_{daily_date}_partial.bz2'))
                daily_data = []
                current_day = file_date
                print(f'Average processing_datafile execution time was {round(np.mean(lstTimes),2)} sec')
                lstTimes = []
            daily_data.append(data)
    # Handle the last batch of files that do not make up a whole day yet
    if daily_data:
        print(f'Saving partial data for date {file_date}..')
        daily_df = pd.concat(daily_data)
        daily_df.reset_index(inplace=True, drop=True)
        daily_df = group_and_combine_columns(daily_df)
        daily_df.to_pickle(cmb(output_subdir, f'{sMeetset}_{file_date}_partial.bz2'))

def combine_columns(df, cols, combined_col_name):
    """
    Combine a list of columns in a DataFrame, giving precedence to non-NaN values.

    This function takes a DataFrame and a list of column names, and combines these columns into a single column.
    The resulting column will have non-NaN values from the first column in the list, and if a value is NaN, it will
    be replaced by the corresponding non-NaN value from the next column in the list, and so on. The combined column
    is returned as a DataFrame with the specified column name.

    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to combine.
    cols (list): A list of column names to combine.
    combined_col_name (tuple): The name for the combined column.
    
    Returns:
    pd.Series: A Series with the combined values.
    """
    combined = df[cols[0]]
    for col in cols[1:]:
        combined = combined.combine_first(df[col])
    return pd.DataFrame({combined_col_name: combined})

def group_and_combine_columns(df, combine_column='Signal No.'):
    """
    Group columns by common indices except for the 'Signal No.' level and combine them.
    
    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to group and combine.
    combine_column (str): The multi index column that should be combined, hence ignored if it has different 
    
    Returns:
    pd.DataFrame: The DataFrame with combined columns.
    """
    # Find the index position of combine_column
    signal_no_index = df.columns.names.index(combine_column)

    # Group columns by common indices except for the signal number
    grouped_columns = {}
    for col in df.columns:
        key = tuple([col[i] for i in range(len(col)) if i != signal_no_index])  # Exclude the signal number index
        if key not in grouped_columns:
            grouped_columns[key] = []
        grouped_columns[key].append(col)

    # Create a list to store the combined columns
    combined_columns = []

    # Combine columns in each group
    for key, cols in grouped_columns.items():
        if len(cols) > 1:
            combined_col_name = key[:signal_no_index] + ('Combined',) + key[signal_no_index:]
            combined_col_df = combine_columns(df, cols, combined_col_name)
        else:
            # Adjust the column name to include 'Combined' in the correct position
            single_col_name = key[:signal_no_index] + ('Combined',) + key[signal_no_index:]
            combined_col_df = df[cols].copy()
            combined_col_df.columns = pd.MultiIndex.from_tuples([single_col_name])
        
        combined_columns.append(combined_col_df)

    # Concatenate all combined columns into a new DataFrame
    combined_df = pd.concat(combined_columns, axis=1)

    # Set the names of the MultiIndex levels
    combined_df.columns.names = df.columns.names

    return combined_df

def read_file(fPath):
    """
    Reads the data rows line by line and processes them into a DataFrame.
    Separately reads the first rows as headers.
    """
    # Open the file in read mode
    with open(fPath, 'r') as file:
        # Read the entire contents of the file into a string
        sData = file.read()
    # Get rid of the trailing separator on each row
    sData = sData.replace(";\n","\n")
    
    # Read the header rows
    header_data = StringIO('\n'.join(sData.split('\n')[:4]))
    header_df = pd.read_csv(header_data, sep=';', header=None)
    
    return sData, header_df


def create_multi_index_df(header_df):
    """
    Creates the multi_index Dataframe from the header rows, returns an empty df with multi-index
    """
    # Create the multi-index
    nSignals = len(header_df.iloc[0])
    # multi_index = pd.MultiIndex.from_arrays([header_df.iloc[1], header_df.iloc[2], header_df.iloc[3], header_df.iloc[0]], names=['Signal Name', 'Unit', 'Tag', 'Signal No.'])
    
    # Below is the option for a MultiIndex that stores [Value, status_bit] for each signal
    multi_indexValues = pd.MultiIndex.from_arrays([header_df.iloc[1], header_df.iloc[2], header_df.iloc[3], header_df.iloc[0], ['Value']*nSignals], names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    multi_indexStatus = pd.MultiIndex.from_arrays([header_df.iloc[1, 1:], header_df.iloc[2, 1:], header_df.iloc[3, 1:], header_df.iloc[0, 1:], ['Status']*(nSignals-1)], names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    lst = multi_indexValues.to_flat_index().append(multi_indexStatus.to_flat_index())
    multi_index = pd.MultiIndex.from_tuples(lst, names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    
    # Initialize an empty DataFrame with the multi-index columns
    dfEmpty = pd.DataFrame(columns=multi_index)
    return dfEmpty

def process_datafile(sData, dfEmpty, header_df):
    """
    Read the sData string, and use the multi-index empty dataframe to create 
    the full dataframe for each file, with a multi-index header
    """    
    # Make sure the timestamp column is set up as multi-index as well
    sTimestampCol = dfEmpty.columns[0]
    rows = []
    data_rows = sData.split('\n')[4:]
    # Determine the lowest signal no. to use in indexing
    # lstSignalNos = [0] + [int(x[3]) for x in dfEmpty.columns[1:]]
    lstSignalNos = header_df.iloc[0]
    # nSignalNoOffset = min() - 1
    
    # Process each data row
    for cnt, row in enumerate(data_rows):
        try:
            if row.startswith('D'):
                if row.endswith(';'):
                    row = row[:-1]
                parts = row.split(';')
                timestamp = parts[1]
                data_dict = {sTimestampCol: timestamp}
                # Skip row if there are not complete sets of 3 values on each row
                if np.mod(len(parts) - 2, 3) != 0:
                    print(f'data_row[{cnt}] skipped, because amount of data values is not a multiple of 3 [signal_no, value, status]')
                    continue
                elif len(parts) < 5:
                    print(f'data_row[{cnt}] skipped, no valid data found')
                    continue
                for i in range(2, len(parts), 3):
                    if i + 2 < len(parts):
                        signal_number = int(parts[i])
                        if parts[i + 1] in ['', 'T-Mobile  NL', 'LTE CAT M1', 'False']:
                            signal_value = np.nan
                        else:
                            signal_value = float(parts[i + 1])
                        status_bit = int(parts[i + 2])
                        # nCol = lstSignalNos.index(signal_number)
                        nCol = lstSignalNos[lstSignalNos == str(signal_number)].index[0]
                        signal_name = header_df.iloc[1, nCol]
                        unit = header_df.iloc[2, nCol]
                        tag = header_df.iloc[3, nCol]
                        # tag = header_df.iloc[3, signal_number - nSignalNoOffset]
                        data_dict[(signal_name, unit, tag, signal_number, 'Value')] = signal_value
                        data_dict[(signal_name, unit, tag, signal_number, 'Status')] = status_bit
                # df = df.append(data_dict, ignore_index=True)
                rows.append(data_dict)
        except Exception as e:
            print(f'Error processing data_row[{cnt}]: {e}')
            continue
    
    # Create the DataFrame from the list of rows
    df = pd.DataFrame(rows)
    # Set the multi-index columns
    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    return df


def load_data(pPickles, sMeetset=None, sDateStart='2024-01-01', sDateEnd='2025-12-31'):
    print(f'\nLoading stored data from {sDateStart} -1 to {sDateEnd}..')
    # Loading one day before, because of 1-2 hr timedelta from UTC to local time
    tStart = datetime.strptime(sDateStart, '%Y-%m-%d').date()
    tEnd = datetime.strptime(sDateEnd, '%Y-%m-%d').date()
    # Only add files for which the YYYY-mm-dd date falls within tStart to tEnd (inclusive)    pickledir = cmb(pPickles,sMeetset)
    pickledir = cmb(pPickles,sMeetset)
    if not os.path.exists(pickledir):
        os.makedirs(pickledir)

    lstPickles = [f for f in os.listdir(pickledir) if (not f.endswith('partial.bz2') and
                  tStart <= datetime.strptime(f.split('_')[-1][0:10], '%Y-%m-%d').date() <= tEnd) or
                  (f.endswith('partial.bz2') and 
                   tStart <= datetime.strptime(f.split('_')[-2][0:10], '%Y-%m-%d').date() <= tEnd)]
    lstPickleData = []
    for pklfile in lstPickles:
        pkldata = pd.read_pickle(cmb(pPickles,sMeetset,pklfile))
        lstPickleData.append(pkldata)
        print(f'  Loaded file {pklfile}..')
    df = pd.concat(lstPickleData)
    df.reset_index(inplace=True, drop=True)
    return df

def flatten_data(df, bStatus=False, ignore_multi_index_differences=False):
    """
    Group columns by common indices except for the 'Signal No.' level and combine them.
    
    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to group and combine.
    bStatus (bool): Can be set to True. In that cas wil Status and Value be added as seperate columns
    ignore_multi_index_differences (bool): Can be set to True to ignore differences in the multi index. 
    In that case it would be ignored if there are differences in Unit or Tag.
    
    Returns:
    pd.DataFrame: The DataFrame with combined columns.
    """
    print("Flatten data...")
    if ignore_multi_index_differences:
        df = group_and_combine_columns(df, combine_column='Unit')
        df = group_and_combine_columns(df, combine_column='Tag')
    # Drop the 'Type' level from the MultiIndex columns
    flattened_columns = df.columns.droplevel('Type')
    # Remove duplicates from the flattened columns
    unique_columns = flattened_columns.drop_duplicates()
    # Get all unique signal names
    signal_names = df.columns.get_level_values('Signal Name').unique()
    # Check for each signal name whether it appears only once in the unique columns
    for signal_name in signal_names:
        occurrences = unique_columns[unique_columns.get_level_values('Signal Name') == signal_name]
        count = sum(unique_columns.get_level_values('Signal Name') == signal_name)
        if count > 1:
            raise ValueError(f"There are multiple columns for the signal name '{signal_name}': {count}. Occurrences: {occurrences}")

    if bStatus:
        idxFlatHeader = df.columns.get_level_values(0) + ' (' + df.columns.get_level_values(4) + ')'
    else:
        type_no_index = df.columns.names.index('Type')
        df = df.xs('Value', axis=1, level=type_no_index, drop_level=False)
        idxFlatHeader = df.columns.get_level_values(0)
    dfHeaders = df.columns.to_frame(index=False)
    df.columns = idxFlatHeader
    # df.set_index(idxFlatHeader[0], drop=True, inplace=True)
    return df, dfHeaders

def round_to_nearest_15_seconds(timestamp):
    """Round a timestamp to the nearest 15-second interval."""
    return timestamp.round('15s')

def combine_and_sync_rows(df):
    """Process the DataFrame to round 15sec intervals and fill values."""
    print("Combine and sync rows...")
    # Round timestamps to the nearest 15-second interval
    df['Rounded Timestamp'] = df['Timestamp'].apply(round_to_nearest_15_seconds)
    
    # Group by the rounded timestamps and apply forward fill within each group
    grouped = df.groupby('Rounded Timestamp').apply(lambda group: group.ffill().iloc[-1])
    
    # Reset the index to get a DataFrame
    processed_df = grouped.reset_index(drop=True)
    
    # Drop the original 'Timestamp' column and rename 'Rounded Timestamp' to 'Timestamp'
    processed_df = processed_df.drop(columns=['Timestamp'])
    processed_df = processed_df.rename(columns={'Rounded Timestamp': 'Timestamp'})
    
    return processed_df

def add_hours_based_on_dst(df, sDateStart, sDateEnd):
    """
    Adjust the 'Timestamp' column by adding 1 or 2 hours depending on whether 
    it falls in Dutch summer time (DST) or winter time, while keeping timestamps naive.
    The original 'Timestamp' column is retained in the output DataFrame.
    """
    print("Adding Adjusted Timestamp...")

    # Define the Europe/Amsterdam timezone
    amsterdam_tz = pytz.timezone('Europe/Amsterdam')
    
    def adjust_time(timestamp):
        # First, make the timestamp aware in UTC to correctly manage DST changes
        utc_timestamp = timestamp.replace(tzinfo=pytz.UTC)
        
        # Convert to Amsterdam time
        amsterdam_time = utc_timestamp.astimezone(amsterdam_tz)
        
        # Check if it is in DST and adjust accordingly (keeping the result naive)
        if amsterdam_time.dst() != timedelta(0):
            # Summer time (DST)
            return timestamp + timedelta(hours=2)
        else:
            # Winter time (Standard Time)
            return timestamp + timedelta(hours=1)
    
    # Create a copy of the DataFrame to avoid the SettingWithCopyWarning
    df_copy = df.copy()
    
    # Keep the 'Original Timestamp' column as is, and create 'Adjusted Timestamp' column
    df_copy['Adjusted Timestamp'] = df_copy['Timestamp'].apply(adjust_time)
    
    # Filter the DataFrame based on the date window using .loc
    tStart = datetime.strptime(sDateStart, '%Y-%m-%d')
    tEnd = datetime.strptime(sDateEnd, '%Y-%m-%d') + timedelta(days=1)
    
    # Use .loc to filter and avoid warnings
    df_copy = df_copy.loc[(df_copy['Adjusted Timestamp'] >= tStart) & 
                          (df_copy['Adjusted Timestamp'] < tEnd)]
    
    return df_copy

# def add_hours_based_on_dst(df, sDateStart, sDateEnd):
#     """
#     Add 1 hour or 2 hours to the 'Timestamp' column depending on whether it is Dutch summer time or winter time.
#     """
#     print("Add Adjusted Timestamp...")
#     # Define the Europe/Amsterdam timezone
#     amsterdam_tz = pytz.timezone('Europe/Amsterdam')
    
#     def adjust_time(timestamp):
#         # Localize the timestamp to Europe/Amsterdam timezone
#         localized_timestamp = amsterdam_tz.localize(timestamp)
        
#         # Check if the timestamp is in DST
#         if localized_timestamp.dst() != timedelta(0):
#             # Summer time (DST)
#             return timestamp + timedelta(hours=2)
#         else:
#             # Winter time (Standard Time)
#             return timestamp + timedelta(hours=1)
    
#     # Rename the original 'Timestamp' column
#     df = df.rename(columns={'Timestamp': 'Original Timestamp'})
    
#     # Apply the adjust_time function to the 'Original Timestamp' column
#     df['Adjusted Timestamp'] = df['Original Timestamp'].apply(adjust_time)
    
#     # Drop the 'Original Timestamp' column
#     df = df.drop(columns=['Original Timestamp'])
    
#     # With the adjusted timestamp, remove the leading/trailing data that falls outside of the date window
#     tStart = datetime.strptime(sDateStart, '%Y-%m-%d')
#     tEnd = datetime.strptime(sDateEnd, '%Y-%m-%d') + timedelta(days=1)
#     rowsDateWindow = df['Adjusted Timestamp'].between(tStart,tEnd)
#     df = df.loc[df[rowsDateWindow].index[:-1], :]
    
#     return df

def make_totalizers_monotonic(df, columns):
    print('\nChecking df for monotonic increasing counters..')
    for col in columns:
        if df[col].is_monotonic_increasing:
            print(f'Totalizer column {col} is monotonic_increasing.')
            continue
        else:
            print(f'Totalizer column {col} is not monotonic_increasing, correcting..')
            lastValidCounter = df.loc[df.index[0], col]
            for nRow, val in enumerate(df[col]): #range(1, len(df)):
                if val < lastValidCounter:
                    df.loc[df.index[nRow], col] = np.nan
                else:
                    lastValidCounter = val
    df = interpolate_columns(df, columns)
    return df

def interpolate_columns(df, columns, nLimit=None):
    """Interpolate the specified columns in the DataFrame."""
    for col in columns:
        if not 'Timestamp' in col:
            df[col] = df[col].interpolate(method='time', limit=nLimit)
    return df

def process_totalizers(df, totalizer_dict):
    """Process the totalizer columns as specified."""
    for new_col, cols in totalizer_dict.items():
        # Set NaN values at the beginning and end of each column to the first and last non-NaN values
        for col in cols:
            first_valid_index = df[col].first_valid_index()
            last_valid_index = df[col].last_valid_index()
            
            if first_valid_index is not None:
                # first_valid_loc = df.index.get_loc(first_valid_index)
                # df[col].iloc[:first_valid_loc] = df[col].iloc[first_valid_loc]
                df.loc[:first_valid_index, col] = df[col].loc[first_valid_index]
            
            if last_valid_index is not None:
                # last_valid_loc = df.index.get_loc(last_valid_index)
                # df[col].iloc[last_valid_loc + 1:] = df[col].iloc[last_valid_loc]
                df.loc[last_valid_index:, col] = df[col].loc[last_valid_index]

        # Sum the columns to create the new totalizer column
        df[new_col] = df[cols].sum(axis=1)
        
        # Create the _actual column
        df[f'{new_col}_actual'] = df[new_col]
        df[f'{new_col}_actual'] = df[f'{new_col}_actual'].ffill()
        
        # Drop summed column
        df = df.drop(columns=new_col)
    
    # Flatten the list of lists and convert to a set to get unique values
    unique_values_totalizer = set(item for sublist in totalizer_dict.values() for item in sublist)

    # Convert the set back to a list if needed
    unique_values_totalizer_list = list(unique_values_totalizer)
    for col in unique_values_totalizer_list:
        # Drop the original columns
        if col in df.columns:
            df = df.drop(columns=col)
        
    return df

def convert_bits(timestamp, values, unpack_format='<d>', modbus_update=pd.Timestamp('2024-09-03 09:41:00')):
    """Function to convert non-NaN values in the specified columns into a single column."""  
    int_list = [int(v) for v in values if not np.isnan(v)]
    if unpack_format == '<d':
        if len(int_list) != 4:
            return np.nan # Ensure we have exactly 4 values to process
    elif unpack_format.endswith('I'):
        if len(int_list) != 2:
            return np.nan # Ensure we have exactly 2 values to process    

    if timestamp > modbus_update:
        byte_sequence = b''.join(struct.pack('>H', n) for n in int_list)
    else:
        # Pack each 16-bit integer into a 2-byte sequence
        byte_sequence = b''.join(struct.pack('<H', n) for n in int_list)
        
    # Interpret the byte sequence based on the specified unpack format
    if unpack_format == '<d':  # Double-precision float
        return round(struct.unpack('<d', byte_sequence)[0], 3)
    elif unpack_format.endswith('I'):  # 32-bit unsigned integer
        return struct.unpack(unpack_format, byte_sequence)[0]
    else:
        raise ValueError(f"Unsupported unpack format: {unpack_format}")

def convert_to_1_minute_data(df, totalizer_dict):
    """Convert 15-second data to 1-minute data."""
    print("Convert to 1 minute data...")    
    # Set 'Adjusted Timestamp' as the index
    df = df.set_index('Adjusted Timestamp')
    
    # Process the totalizer columns
    df = process_totalizers(df, totalizer_dict)
    
    # Identify non-totalizer columns
    non_totalizer_columns = list(set(df.columns) - set(f'{col}_actual' for col in totalizer_dict.keys()))
    totalizer_columns = [f'{col}_actual' for col in totalizer_dict.keys()]
    # Resample to 1-minute intervals and calculate the mean for normal columns
    #df_1min_non_totalizer = df[non_totalizer_columns].resample('1min', origin='end', offset='15s').mean()
    df_1min_non_totalizer = df[non_totalizer_columns].resample('1min').mean()

    # For totalizer columns, take the value at the zero second within that minute
    #df_1min_totalizer = df[totalizer_columns].resample('1min', origin='end', offset='15s').first()
    df_1min_totalizer = df[totalizer_columns].resample('1min').first()

    # Combine the two DataFrames
    df_1min = pd.concat([df_1min_non_totalizer, df_1min_totalizer], axis=1)

    # Calculate the _diff columns using forward fill to handle NaNs
    for col in totalizer_columns:
        diff_col = col.replace('_actual', '_diff')
        df_1min[diff_col] = df_1min[col].ffill().diff().shift(-1).fillna(0)

    # Reset the index to make 'Adjusted Timestamp' a column again
    df_1min = df_1min.reset_index()
    
    return df_1min

def has_nan_streak(series, threshold=5):
    # Identify consecutive NaN streaks
    streak = series.isna().astype(int).groupby(series.notna().cumsum()).sum()
    # Return True if any streak is greater than or equal to the threshold
    return streak.max() >= threshold

def convert_to_1_hour_data(df_1min):
    df_1hr = pd.DataFrame(columns=df_1min.columns)
    
    # Identify column groups
    colsActual = [col for col in df_1min.columns if col.endswith('_actual')] + ['Adjusted Timestamp']
    colsDiff = [col for col in df_1min.columns if col.endswith('_diff')]
    colsMean = list(set(df_1min.columns) - set(colsActual) - set(colsDiff))
    
    # Resample mean columns (check for 5-minute NaN streaks before calculating the mean)
    def custom_mean(x):
        if has_nan_streak(x, 5):  # If 5 or more consecutive NaNs, return NaN for the hour
            return pd.NA
        return x.mean()

    #df_1hr[colsMean] = df_1min[colsMean].resample('h', origin='end', offset='1min').apply(custom_mean)
    df_1hr[colsMean] = df_1min[colsMean].resample('h').apply(custom_mean)
    
    # Resample actual columns (first value per hour)
    #df_1hr[colsActual] = df_1min[colsActual].resample('h', origin='end', offset='1min').first()
    df_1hr[colsActual] = df_1min[colsActual].resample('h').first()
    # Resample diff columns (difference between first and last actual values, respecting NaN streaks)
    for col in colsDiff:
        actual_col = col[:-5] + '_actual'  # Strip '_diff' and add '_actual'
        if actual_col in colsActual:
            def custom_sum(x):
                if has_nan_streak(x, 5):
                    return pd.NA
                return x.sum()
#            df_1hr[col] = df_1min[actual_col].resample('h', origin='end', offset='1min').apply(
#                lambda x: (x[-1] - x[0]) if not has_nan_streak(x, 5) else pd.NA
#            )
            df_1hr[col] = df_1min[col].resample('h').apply(custom_sum)

    
    return df_1hr


def interpolate(series, thresh=20):
    # where the nan values are
    nans = series.isna()

    # calculate the size of consecutive `nan`
    mask = nans.groupby([(~nans).cumsum(),nans]).transform('size') > thresh
    return series.interpolate(method='linear', limit_area='inside').mask(mask)

def get_start_summer_time(year):
    summer_start_date = datetime(year, 3, 31)
    while summer_start_date.weekday() != 6:  # 6 is Sunday
        summer_start_date -= timedelta(days=1)
    return summer_start_date

def get_start_winter_time(year):
    winter_start_date = datetime(year, 10, 31)
    while winter_start_date.weekday() != 6:  # 6 is Sunday
        winter_start_date -= timedelta(days=1)
    return winter_start_date

def check_monotonic_and_fill_gaps(df, freq='15s', tolerance_hours=2):
    def remove_duplicated_winter_time(df):
        min_year = df.index.min().year
        max_year = df.index.max().year

        # Initialize an empty mask
        overlap_hour_mask = pd.Series(False, index=df.index)

        # Iterate over the range of years and create the mask
        for year in range(min_year, max_year + 1):
            winter_start_date = get_start_winter_time(year)
            yearly_mask = (df.index.date == winter_start_date.date()) & (df.index.hour == 2)
            overlap_hour_mask = overlap_hour_mask | yearly_mask
        
        # Get all indices within the overlapping hour
        overlapping_indices = df.index[overlap_hour_mask]
        
        # Create a boolean mask for unique indices only
        unique_indices_mask = ~overlapping_indices.duplicated(keep='first')
        
        # Create a DataFrame that includes only the unique overlapping entries
        df_unique = df[overlap_hour_mask][unique_indices_mask]

        # Include non-overlapping data in the resulting DataFrame
        df_result = pd.concat([df[~overlap_hour_mask], df_unique])

        # Sort the resulting DataFrame by index
        df_result = df_result.sort_index()
        return df_result
    
    # Define the Amsterdam timezone for DST checks
    amsterdam_tz = pytz.timezone('Europe/Amsterdam')

    # Get the date where the summer time (DST) starts this year
    this_year = datetime.now().year
    summer_start_date = get_start_summer_time(this_year)
    
    # Get the date where the winter time starts this year and years in the past within the dataframe
    this_year = datetime.now().year
    winter_start_date = get_start_winter_time(this_year)

    # Remove duplicates from the DataFrame index on the winter_start_date
    df = remove_duplicated_winter_time(df)

    # Check if the DataFrame index is monotonic increasing
    if not df.index.is_monotonic_increasing:
        non_monotonic_index = (df.index[1:] - df.index[:-1]) < pd.Timedelta(0)
        non_monotonic_index = np.insert(non_monotonic_index, 0, False)
        non_monotonic_datetimes = df.index[non_monotonic_index]
        for non_monotonic_datetime in non_monotonic_datetimes:
            # Check if non-monotonicity occurs on the exact hour of the time change
            if non_monotonic_datetime.date() in [summer_start_date.date(), winter_start_date.date()] and non_monotonic_datetime.hour == 2:
                pass
            else:
                raise ValueError("Index is not monotonic increasing and not due to change of summer/winter time.")

    # Generate the expected range based on the start and end, with the given frequency
    expected_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq=freq)

    # Remove duplicates from the expected range
    expected_range = expected_range[~expected_range.duplicated()]

    # Reindex the DataFrame to the expected range, filling gaps with NaNs
    df = df.reindex(expected_range)

    return df

def interpolate_nans(df, nLimit=None):
    print('\nInterpolating missing values in df. Current fraction of missing values:')
    dfMissing = pd.DataFrame()
    dfMissing['Sum'] = df.isna().sum()
    dfMissing['Fraction'] = round(df.isna().sum() / df.isna().count(),3)
    print(dfMissing)
    df = df.apply(interpolate, thresh=nLimit)
    # And deal with the first rows as well:
    df.iloc[:nLimit, 3:] = df.iloc[:nLimit, 3:].bfill(axis=0)
    return df, dfMissing

def calculate_heat_flow(df_1min, col_wm_flow, col_flow, col_tempout, col_tempin):
    df_1min = df_1min.copy()
    required_cols = [col_flow, col_tempout, col_tempin]
    if not all(col in df_1min.columns for col in required_cols):
        print("One or more required columns are missing. Skipping calculation.")
        return df_1min
    temp_diff = df_1min[col_tempout] - df_1min[col_tempin]
    df_1min[col_wm_flow] = (df_1min[col_flow] * spec_heat_water * (temp_diff)).div(3600)
    return df_1min


def sortColumns(df, lstStartCols):
    lstToSort = list(set(df.columns) - set(lstStartCols))
    lstToSort.sort()
    lstNewOrder = lstStartCols + lstToSort
    df = df[lstNewOrder]
    return df

def add_cop_values(df, rows_to_process=None):
    """
    Calculate the COP values for the specified rows in the DataFrame.
    If no rows are specified, calculate the COP values for all rows.
    """
    # Create a copy of the input DataFrame to avoid modifying the original data
    df = df.copy()
    
    # If rows_to_process is not specified, process all rows
    if rows_to_process is None:
        rows_to_process = df.index
    
    # Extract the outdoor air temperature from the DataFrame
    t_omg = df.loc[rows_to_process, 'Weather Temp Air']
    # Extract the internal temperature from the DataFrame
    t_wm = df.loc[rows_to_process, 'Belimo03 Temp2 internal']
    # Calculate the temperature difference between internal and outdoor air
    diff_temp = t_wm - t_omg
    # Calculate the percentage of total power for Eastron01 and Eastron02
    pe_wp1 = df['Eastron01 Total Power']
    pe_wp2 = df['Eastron02 Total Power']
    t_wp_uit = df['Belimo03 Temp2 internal']
    pct_wm1 = pe_wp1/(16000*(0.0053*t_wp_uit**2 + 0.1207*t_wp_uit + 11.914)/35.6)
    pct_wm2 = pe_wp2/(23000*(0.0053*t_wp_uit**2 + 0.1207*t_wp_uit + 11.914)/35.6)
    # pct_wm1 = df.loc[rows_to_process, 'Eastron01 Total Power'].div(max_power_wm1) no longer applicable
    # pct_wm2 = df.loc[rows_to_process, 'Eastron02 Total Power'].div(max_power_wm2) no longer applicable
    # Calculate the coefficient of performance (COP) for wm1
    cop_fabr1 = pd.Series(diff_temp * ((isi + t_omg * iss) + pct_wm1 * (ssi + t_omg * sss)) + (iii + t_omg * iis) + pct_wm1 * (sii + t_omg * sis), index=rows_to_process)
    # Calculate the coefficient of performance (COP) for wm2
    cop_fabr2 = pd.Series(diff_temp * ((isi + t_omg * iss) + pct_wm2 * (ssi + t_omg * sss)) + (iii + t_omg * iis) + pct_wm2 * (sii + t_omg * sis), index=rows_to_process)
    # Set COP_fabr1 to NaN if Eastron01 Total Power is below 300 We
    # cop_fabr1[df.loc[rows_to_process, 'Eastron01 Total Power'] < minimum_WP_power] = np.nan
    # # Set COP_fabr2 to NaN if Eastron02 Total Power is below 300 We
    # cop_fabr2[df.loc[rows_to_process, 'Eastron02 Total Power'] < minimum_WP_power] = np.nan
    # Add the calculated COP values to the DataFrame
    df.loc[rows_to_process, 'COP_fabr1'] = cop_fabr1
    df.loc[rows_to_process, 'COP_fabr2'] = cop_fabr2
    
    # Calculate the total power
    total_power = df.loc[rows_to_process, 'Eastron01 Total Power'] + df.loc[rows_to_process, 'Eastron02 Total Power']
    
    # Create a mask for valid total power (non-zero)
    valid_power_mask = total_power != 0
    
    # Calculate the weighted mean of COP_fabr1 and COP_fabr2 only for valid total power
    cop_fabr1_filled = np.nan_to_num(df.loc[rows_to_process, 'COP_fabr1'])
    cop_fabr2_filled = np.nan_to_num(df.loc[rows_to_process, 'COP_fabr2'])
    total_power_filled = np.nan_to_num(df.loc[rows_to_process, 'Eastron01 Total Power']) + np.nan_to_num(df.loc[rows_to_process, 'Eastron02 Total Power'])
    
    weighted_cop_fabr = np.where(
        valid_power_mask,
        (cop_fabr1_filled * np.nan_to_num(df.loc[rows_to_process, 'Eastron01 Total Power']) + cop_fabr2_filled * np.nan_to_num(df.loc[rows_to_process, 'Eastron02 Total Power'])) / total_power_filled,
        np.nan
    )
    
    # Ensure the relevant columns are numeric
    df['COP_fabr1'] = pd.to_numeric(df['COP_fabr1'], errors='coerce')
    df['COP_fabr2'] = pd.to_numeric(df['COP_fabr2'], errors='coerce')
    df['Eastron01 Total Power'] = pd.to_numeric(df['Eastron01 Total Power'], errors='coerce')
    df['Eastron02 Total Power'] = pd.to_numeric(df['Eastron02 Total Power'], errors='coerce')
    
    
    # Handle cases where one of the COP values is NaN
    weighted_cop_fabr = np.where(
        np.isnan(df.loc[rows_to_process, 'COP_fabr1']) & ~np.isnan(df.loc[rows_to_process, 'COP_fabr2']),
        df.loc[rows_to_process, 'COP_fabr2'],
        weighted_cop_fabr
    )
    weighted_cop_fabr = np.where(
        ~np.isnan(df.loc[rows_to_process, 'COP_fabr1']) & np.isnan(df.loc[rows_to_process, 'COP_fabr2']),
        df.loc[rows_to_process, 'COP_fabr1'],
        weighted_cop_fabr
    )
    
    # Ensure COP_fabr is NaN if both COP_fabr1 and COP_fabr2 are NaN
    weighted_cop_fabr = np.where(
        np.isnan(df.loc[rows_to_process, 'COP_fabr1']) & np.isnan(df.loc[rows_to_process, 'COP_fabr2']),
        np.nan,
        weighted_cop_fabr
    )
    
    # Add the weighted COP to the DataFrame
    df.loc[rows_to_process, 'COP_fabr'] = weighted_cop_fabr

    # Add the calculated Q_fabr1 and Q_fabr2 to the DataFrame
    df.loc[rows_to_process, 'Q_fabr1'] = df.loc[rows_to_process, 'COP_fabr1'] * df.loc[rows_to_process, 'Eastron01 Total Power']
    df.loc[rows_to_process, 'Q_fabr2'] = df.loc[rows_to_process, 'COP_fabr2'] * df.loc[rows_to_process, 'Eastron02 Total Power']
    df.loc[rows_to_process, 'Q_fabrikant'] = df.loc[rows_to_process, 'Q_fabr1'].fillna(0) + df.loc[rows_to_process, 'Q_fabr2'].fillna(0)
    # Return the modified DataFrame
    return df

def remove_outliers(df, df_minmax, lstHeaderMapping, max_consecutive=5):
    outliers_count = {}
    max_consecutive_count = {}

    for column in df.columns:
        if column in lstHeaderMapping:
            mv_column = lstHeaderMapping[column][0]
            if mv_column in df_minmax.columns:

                # Fetch the min and max values from df_minmax
                try:
                    min_val = df_minmax.loc[2, mv_column]
                    max_val = df_minmax.loc[3, mv_column]
                except KeyError:
                    print(f"Warning: {mv_column} not found in df_minmax. Skipping column {column}.")
                    continue

                # Create mask for outliers
                outliers = (df[column] < min_val) | (df[column] > max_val)

                # Track the number of outliers for this column
                outliers_count[column] = outliers.sum()

                # Initialize the max_consecutive_count for this column
                max_consecutive_count[column] = 0

                # Group consecutive outliers (reset group numbers when non-outliers appear)
                group = (outliers.diff().ne(0).cumsum()) * outliers
                
                if len(group.unique()) > 1:
                    # Print that this column has outliers
                    print(f"Column '{column}' has {outliers_count[column]} outliers.")

                for grp_num, grp_indices in df[column].groupby(group).groups.items():
                    if grp_num == 0:  # Skip non-outlier groups
                        continue
                    
                    if len(grp_indices) > max_consecutive:  # Set NaN if more than max_consecutive outliers
                        df.loc[grp_indices, column] = np.nan
                        max_consecutive_count[column] += 1  # Increment the count for this column
                    else:  # Replace 1 to max_consecutive outliers with the average of surrounding values
                        first_idx = df.index.get_loc(grp_indices[0])
                        last_idx = df.index.get_loc(grp_indices[-1])
                        
                        # Get the values before and after the sequence based on position
                        before_val = df[column].iloc[first_idx - 1] if first_idx > 0 else np.nan
                        after_val = df[column].iloc[last_idx + 1] if last_idx < len(df) - 1 else np.nan
                        
                        fill_value = np.nanmean([before_val, after_val])
                        df.loc[grp_indices, column] = fill_value

    return df, outliers_count, max_consecutive_count

def copy_output_to_automaticreporting(weeks_with_year):
    src_dir = os.path.join(pRV, sMeetsetFolder)
    dst_dir = os.path.join(pBase, 'Automatic excel calculations', 'Input', sMeetsetFolder)

    for dirpath, dirnames, filenames in os.walk(src_dir):
        print(dirpath)
        rel_path = os.path.relpath(dirpath, src_dir)
        if rel_path != '.':
            dst_path = os.path.join(dst_dir, rel_path)
            if not os.path.exists(dst_path):
                os.makedirs(dst_path)
            else:
                # Remove all files in the destination directory
                for file in os.listdir(dst_path):
                    os.remove(os.path.join(dst_path, file))
            for filename in filenames:
                src_file = os.path.join(dirpath, filename)
                dst_file = os.path.join(dst_path, filename)
                shutil.copy2(src_file, dst_file)

def combine_raw_columns(df):
    modbus_update = pd.Timestamp('2024-09-03 09:41:00') # From here on there was a modbus firmware update, changing endian for bit conversion
    cutoff_date = pd.Timestamp('2025-06-23') # From here on, the modbus firware update is applied, from now on bit conversion is no longer needed

    # Clean up redundant columns
    weatherAirTempCols = [col for col in df.columns if col.startswith('Weather Temp Air')]
    df.drop(weatherAirTempCols[1:], axis=1, inplace=True)
    for col in ['Itron Gas volume 2', 'Itron Gas volume 3']:
        if col in df.columns:
            df.drop(col, axis=1, inplace=True)

    dictBitConversionEVHI = {
        'Stream1 Pressure': ['Stream1 PressureA', 'Stream1 PressureB', 'Stream1 PressureC', 'Stream1 PressureD'],
        'Stream1 Temperature': ['Stream1 TemperatureA', 'Stream1 TemperatureB', 'Stream1 TemperatureC', 'Stream1 TemperatureD'],
        'Stream1 Flow': ['Stream1 FlowA', 'Stream1 FlowB', 'Stream1 FlowC', 'Stream1 FlowD'],
        'Stream2 Pressure': ['Stream2 PressureA', 'Stream2 PressureB', 'Stream2 PressureC', 'Stream2 PressureD'],
        'Stream2 Temperature': ['Stream2 TemperatureA', 'Stream2 TemperatureB', 'Stream2 TemperatureC', 'Stream2 TemperatureD'],
        'Stream2 Flow': ['Stream2 FlowA', 'Stream2 FlowB', 'Stream2 FlowC', 'Stream2 FlowD'],
    }

    print("Convert bits to values...")

    for key, cols in dictBitConversionEVHI.items():
        if cols[0] in df.columns:
            mask = df['Timestamp'] < cutoff_date
            timestamps = df.loc[mask, 'Timestamp'].to_numpy()
            values_array = df.loc[mask, cols].to_numpy()

            results = [
                convert_bits(ts, vals, unpack_format='<d', modbus_update=modbus_update)
                for ts, vals in zip(timestamps, values_array)
            ]

            df.loc[mask, key] = results
            df.drop(cols, axis=1, inplace=True)

    return df



def process_weather_temp_air(series):
    # Create a copy of the series to avoid modifying the original data
    series = series.copy()

    # Identify blocks of NaNs and their lengths
    nan_blocks = series.isna().astype(int).groupby(series.notna().cumsum()).transform('sum')

    # Iterate through each value in the series
    for i in range(len(series)):
        # Skip if the current value is part of a NaN block of 180 rows or more
        if nan_blocks[i] >= 180:
            continue

        # Get the 5 values before and 5 values after, ignoring NaNs
        before_values = series[max(0, i - 5):i].dropna()
        after_values = series[i + 1:i + 6].dropna()

        # Combine the before and after values
        surrounding_values = pd.concat([before_values, after_values])

        # If there are no surrounding values, skip this iteration
        if surrounding_values.empty:
            continue

        # Calculate the average of the surrounding values
        surrounding_mean = surrounding_values.mean()

        # Check if the current value is more than 1 higher or 1 lower than the surrounding mean
        if not pd.isna(series[i]) and abs(series[i] - surrounding_mean) > 1:
            # Replace the current value with the surrounding mean
            series.iloc[i] = surrounding_mean

    # Fill NaNs with interpolation only for blocks smaller than 180 rows
    series = series.interpolate(method='linear', limit_area='inside')

    # Fill remaining NaNs at the beginning or end of the column
    series = series.fillna(method='bfill').fillna(method='ffill')

    return series

def name_highest_belimos(df):
    pat = re.compile(r'^Belimo0*(\d+)\b')
    belimo_cols = [c for c in df.columns if pat.search(c)]
    nums = sorted({int(pat.search(c).group(1)) for c in belimo_cols})

    if len(nums) < 2:
        raise Exception("Not enough Belimos to find highest + 2nd highest.")
    highest = nums[-1]
    second_highest = nums[-2]

    for c in belimo_cols:
        m = pat.search(c)
        n = int(m.group(1))
        if n == highest:
            new_name = pat.sub('Belimo-highest', c, count=1)
            df[new_name] = df[c]
        elif n == second_highest:
            new_name = pat.sub('Belimo-2highest', c, count=1)
            df[new_name] = df[c]

    return df

if __name__ == "__main__":
    # Set environment variables
    pBase = os.getcwd()
    pParentDir = os.path.dirname(pBase)
    pInput = cmb(pParentDir,'Collected Data')
    pPickles = cmb(pBase,'ImportedPickles')
    pRaw = cmb(pParentDir,'01. Excel data overview')
    pRV = cmb(pParentDir,'02. Excel MV-RV')
    pWord = cmb(pParentDir,'03. Word results')
    pRVMeetFolder = cmb(pRV, sMeetsetFolder)
    year = sDateStart.split('-')[0]

    
    if bReadPickles: 
        raise ValueError("Read pickles is set to True, but this is deprecated. Please set it to False.")
        # # Note that the function convert_excel_output sometimes results in errors.
        # # This happens when a keyboard or mouse click is interrupting the process when excel is opened by the code.
        # # Therefore the results are stored and can be read from pickles, but this is overwritten everytime.
        # # Thefore make sure you have just runned the code with the correct settings before reading the pickles.
        # variables = read_variables(var_names)
        # # change_files = variables['change_files']
        # df_1hr_newheaders = variables['df_1hr_newheaders']
        # sMeetsetFolder = variables['sMeetsetFolder']
        # df_1min_newheaders = variables['df_1min_newheaders']
        # outliers_count = variables['outliers_count']
        # # convert_excel_output(pBase, change_files)
        
        # prefix = '1hour - RV - '
        # weeks_with_year = add_enthalpy_calcualations(df_1hr_newheaders, pRVMeetFolder, year, prefix=prefix)
        # prefix = '1min - RV - '
        # # weeks_with_year = add_enthalpy_calcualations(df_1min_newheaders, pRVMeetFolder, year, prefix=prefix)
        # copy_output_to_automaticreporting(weeks_with_year)
        # knmi_data_used = False
        # create_word_documents(sMeetsetFolder, location, weeks_with_year, knmi_data_used, pWord)

    else:
        
        # Read data into pickles
        if bReadData:
            process_and_save(pBase, pInput, pPickles, sMeetsetFolder)

        dfRaw = load_data(pPickles, sMeetset=sMeetsetFolder, sDateStart = sDateStart, sDateEnd = sDateEnd)
        df, dfHeaders = flatten_data(dfRaw, bStatus=False, ignore_multi_index_differences=True)
        df = combine_raw_columns(df)

        if bDebugStopExecutionHere: 
            print('Stopping here for debugging')   
            raise ValueError("Stopping here for debugging..")
        else:
            df = combine_and_sync_rows(df)
            df = add_hours_based_on_dst(df, sDateStart, sDateEnd)

        
        # Exctract the keys from header list that are needed for this code to run
        # Remove diff if added, to get the original columns
        columns_keys = list(hHP.makeAllHeaderMappings().keys())
        original_columns = [item.replace('_diff', '') if item.endswith('_diff') else item for item in columns_keys]
        # Some columns are created, ignore these
        ignore_columns = ['Contains missing data', 'Missing data (no Eastron02)', 
                        'Missing data (no Belimo)', 'Eastron Total Power']
        needed_columns = list(set(original_columns)- set(ignore_columns))
        # Check that only existing columns remain 
        needed_columns = list(set(needed_columns).intersection(set(df.columns)))
        # Some columns are a lot of times NaN, include those
        ignore_eastron2_column = [col for col in needed_columns if col.startswith('Eastron02')]
        needed_columns_without_eastron2 = list(set(needed_columns)- set(ignore_eastron2_column))
        exclude_belimo_columns = [col for col in needed_columns if col.startswith('Belimo')]
        needed_columns_without_belimo = list(set(needed_columns_without_eastron2)- set(exclude_belimo_columns))
        # df['Contains missing data'] = df[needed_columns].isnull().any(axis=1)
        df['Missing data (no Eastron02)'] = df[needed_columns_without_eastron2].isnull().any(axis=1)
        df['Missing data (no Belimo)'] = df[needed_columns_without_belimo].isnull().any(axis=1)

        df = sortColumns(df, ['Adjusted Timestamp','Missing data (no Eastron02)','Missing data (no Belimo)'])
        # Write to Excel the full dataframe
        print('Saving full dataframe...')
        df.set_index('Adjusted Timestamp', drop=False, inplace=True)
        
        df_soil = get_soil_temp_full()
        df_soil_indexed = df_soil.set_index('datetime')
        df = df.copy()
        df.loc[:, 'Soil temperature corrected'] = (
            (df_soil_indexed['TB5'] + KNMI_SOIL_CORRECTION)
            .reindex(df['Adjusted Timestamp'])
            .interpolate(method='time')
            .ffill()
            .bfill()
            .values
        )

        # Save the data before correcting for outliers and interpolating missing values
        filename = 'Before_corrections ' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx') + '.xlsx'
        df = check_monotonic_and_fill_gaps(df, freq='15s')
        filedir = cmb(pRaw, sMeetsetFolder)
        filepath = cmb(pRaw, sMeetsetFolder, filename)
        if not os.path.exists(filedir):
            os.makedirs(filedir)
        df.to_excel(filepath)
        
        df, dfMissing = interpolate_nans(df, nLimit=maximum_interpolate_nans)
        # First interpolate, then make sum of Eastron power. Otherwise, some interpolated values are missing in the sum.
        start_time = (df.index[0] - timedelta(days=1)).strftime('%Y%m%d%H')
        end_time = (df.index[-1] + timedelta(days=1)).strftime('%Y%m%d%H')
        df_knmi = get_hour_data_dataframe(stations=[knmi_station], start=start_time, end=end_time, variables=['T', 'P', 'U'])
        df_knmi['P_bar'] = df_knmi['P'] / 10000
        p_atmospheric_interp = df_knmi['P_bar'].reindex(df.index).interpolate(method='time')
        if 'Weather Abs Air Pressure' in df.columns:
            # Convert pgasin from barg to bara, but if air pressure is below minimum_atmospheric_pressure mbar, add atmospheric_pressure to the value instead of reading the air pressure
            df['PgasIn'] = np.where(
                df['Weather Abs Air Pressure'].fillna(p_atmospheric_interp*1000) < minimum_atmospheric_pressure,
                p_atmospheric_interp + df['PgasIn'],
                df['Weather Abs Air Pressure'].fillna(p_atmospheric_interp*1000).div(1000) + df['PgasIn']
            )
        else:
            df['PgasIn'] = p_atmospheric_interp + df['PgasIn']
        if 'Eastron01 Total Power' in df.columns and 'Eastron02 Total Power' in df.columns:
            df['Eastron Total Power'] = df[['Eastron01 Total Power', 'Eastron02 Total Power']].sum(axis=1)
            rowsMaskNAN = df[['Eastron01 Total Power', 'Eastron02 Total Power']].isna().all(axis=1)
            df.loc[df[rowsMaskNAN].index, 'Eastron Total Power'] = np.nan

        # Check on outlier values:
        # TgasIn: Outliers for temperature and PgasIn: outliers for incoming pressure
        # UPDATE, convert Soil temperature corrected for TgasIn
        if location == 'Deventer':
            df['TgasIn'] = df['Soil temperature corrected'] 
        rowsMask = df[~df['TgasIn'].between(TgasIn_min, TgasIn_max)].index
        df.loc[rowsMask, 'TgasIn'] = np.nan
        rowsMask = df[~df['PgasIn'].between(PgasIn_min, PgasIn_max)].index
        df.loc[rowsMask, 'PgasIn'] = np.nan
        
        # Handle totalizers, make them consistent and then use for 1min output.
        # Fixed columns 
        fixed_cols = ['Itron Gas volume 1']
        # Dynamically find columns containing 'FlowTotalL'
        flow_cols = [col for col in df.columns if 'FlowTotalL' in col]
        # Build dict_totalizer
        dict_totalizer = {col: [col] for col in fixed_cols + flow_cols}
        dict_totalizer = {k: dict_totalizer[k] for k in dict_totalizer if k in df.columns}
        colsTotalizer = [item for sublist in list(dict_totalizer.values()) for item in sublist]
        df = make_totalizers_monotonic(df, colsTotalizer)
        
        # Remove lines where no data was logged, so check a few basic columns are all nan:
        dropIndices = df[df[['Missing data (no Belimo)','PgasIn','TgasIn']].isna().all(axis=1)].index        
        
        df.drop(dropIndices.unique(), axis=0, inplace=True)

        #Create new columns for the highest and second highest Belimo columns
        if not bDebugStopExecutionHere:
            df = name_highest_belimos(df)

        # Now work on the 1min output for the energy balance calculation    
        df_1min = convert_to_1_minute_data(df, dict_totalizer)
        df_1min.set_index('Adjusted Timestamp', drop=False, inplace=True)

        if 'Weather Temp Air' in df_1min.columns:
            df_1min['Weather Temp Air'] = process_weather_temp_air(df_1min['Weather Temp Air'])
        if 'Itron Gas volume 1_diff' in df_1min.columns:
            # df_1min['Itron Gas volume 1_diff'] = df_1min['Itron Gas volume 1_diff']*60*1000 # Convert from m3/minute to l/h
            df_1min['Itron Gas volume 1_diff'] = df_1min['Itron Gas volume 1_diff']*1000 # Convert from m3/minute to l/h
        if 'Belimo03 FlowTotalL_diff' in df_1min.columns:
            print('No change for Belimo')
            # df_1min['Belimo01 FlowTotalL_diff'] = df_1min['Belimo01 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['Belimo02 FlowTotalL_diff'] = df_1min['Belimo02 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['Belimo03 FlowTotalL_diff'] = df_1min['Belimo03 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['BelimoValve FlowTotalL_diff'] = df_1min['BelimoValve FlowTotalL_diff']*60 # Convert from litre/minute to to l/h

        # DONE here for older data sets: copy flow rates where _diff exists and 'FlowRate' is empty, to FlowRate
        if bDebugStopExecutionHere:
            belimoCorr = ['01','02','03','Valve']
            for sNum in belimoCorr:
                rowsNoFlow =  df_1min[df_1min[f'Belimo{sNum} FlowRate'].isna()].index
                rowsWithData = df_1min[df_1min[[f'Belimo{sNum} Temp1 external',f'Belimo{sNum} Temp2 internal',f'Belimo{sNum} FlowTotalL_diff']].notna().all(axis=1)].index
                rowsMask = rowsNoFlow.intersection(rowsWithData)
                df_1min.loc[rowsMask, f'Belimo{sNum} FlowRate'] = df_1min.loc[rowsMask,f'Belimo{sNum} FlowTotalL_diff']

        # lstBelimoTotals = ['Belimo01 FlowTotalM3', 'Belimo02 FlowTotalM3', 'Belimo03 FlowTotalM3']
        # for col in lstBelimoTotals:
        #     if col in df_1min.columns:
        #         df_1min[col] = df_1min[col]*60*10 # Convert first from m3*100/minute to m3/h and than to l/h
        df_1min = sortColumns(df_1min, ['Adjusted Timestamp','Missing data (no Eastron02)','Missing data (no Belimo)'])
        minmax_filepath = cmb(pBase, "minmax_cols.xlsx")
        df_minmax = pd.read_excel(minmax_filepath)
        df_1min, outliers_count, max_consecutive_count = remove_outliers(df_1min, df_minmax, lstHeaderMapping=hHP.makeAllHeaderMappings())

        # Add some columns with weighted mean flow rates
        df_1min = calculate_heat_flow(df_1min, col_wm_flow='Q_ket1_wm', col_flow='Belimo01 FlowRate', col_tempout='Belimo01 Temp2 internal', col_tempin='Belimo01 Temp1 external') #Note that we use for Q_ket in enthalpy calculations now OV, so that should be compared with Q_OV_wm instead of Q_ket1_wm!
        df_1min = calculate_heat_flow(df_1min, col_wm_flow='Q_OV_wm', col_flow='Belimo-2highest FlowRate', col_tempout='Belimo-2highest Temp2 internal', col_tempin='Belimo-2highest Temp1 external')
        df_1min = calculate_heat_flow(df_1min, col_wm_flow='Q_WP_wm', col_flow='Belimo-highest FlowRate', col_tempout='Belimo-highest Temp2 internal', col_tempin='Belimo-highest Temp1 external')

        df_1hr = convert_to_1_hour_data(df_1min)

        if df_1hr['Itron Gas volume 1_diff'].isna().sum() == len(df_1hr):
            df_1hr['Itron Gas volume 1_diff'] = df_1hr['Itron Gas volume 1_actual'].ffill().diff().shift(-1).fillna(0)
            #df_1hr['Itron Gas volume 1_diff'] = df_1hr['Itron Gas volume 1_actual'].ffill().diff().fillna(0)

        # Fill missing weather data with KNMI data:
        # Construct start and end time for knmi data
        # Record whether we use knmi data by comparing number of NaN values before and after filling
        # Sum of NaN values before filling in weather data
        if 'Weather Temp Air' not in df_1hr.columns or 'Weather Abs Air Pressure' not in df_1hr.columns or 'Weather Rel Humidity' not in df_1hr.columns:
            print("Weather data columns are missing in the 1 hour data and are filled with knmi data.")
            knmi_data_used = True
            missing_weather_columns = True
            # Add missing columns with NaN values
            if 'Weather Temp Air' not in df_1hr.columns:
                df_1hr['Weather Temp Air'] = np.nan
            if 'Weather Abs Air Pressure' not in df_1hr.columns:
                df_1hr['Weather Abs Air Pressure'] = np.nan
            if 'Weather Rel Humidity' not in df_1hr.columns:
                df_1hr['Weather Rel Humidity'] = np.nan
        else:
            missing_weather_columns = False
            number_of_nan_values_before = df_1hr['Weather Temp Air'].isna().sum() + df_1hr['Weather Abs Air Pressure'].isna().sum() + df_1hr['Weather Rel Humidity'].isna().sum()
            knmi_data_used = False
        start_time = (df_1hr.index[0] - timedelta(days=1)).strftime('%Y%m%d%H')
        end_time = (df_1hr.index[-1] + timedelta(days=1)).strftime('%Y%m%d%H')
        df_knmi = get_hour_data_dataframe(stations=[knmi_station], start=start_time, end=end_time, variables=['T', 'P', 'U'])

        # Fill NaN values with KNMI data
        temp_air_filled = (df_knmi['T'] / 10).reindex(df_1hr.index)
        air_pres_filled = (df_knmi['P'] / 10).reindex(df_1hr.index)
        rel_hum_filled = (df_knmi['U']).reindex(df_1hr.index)

        # Replace NaN values or values with a difference greater than 10 degrees
        hours_knmi_used = 0
        for hour in df_1hr.index:
            if pd.isna(df_1hr.at[hour, 'Weather Temp Air']) or abs(df_1hr.at[hour, 'Weather Temp Air'] - temp_air_filled.at[hour]) > 10:
                # Replace the hourly value in df_1hr with KNMI data
                df_1hr.at[hour, 'Weather Temp Air'] = temp_air_filled.at[hour]
                df_1hr.at[hour, 'Weather Abs Air Pressure'] = air_pres_filled.at[hour]
                df_1hr.at[hour, 'Weather Rel Humidity'] = rel_hum_filled.at[hour]
                hours_knmi_used += 1

                # Replace the corresponding values in df_1min for the same hour
                df_1min.loc[df_1min.index.floor('h') == hour, 'Weather Temp Air'] = temp_air_filled.at[hour]
                df_1min.loc[df_1min.index.floor('h') == hour, 'Weather Abs Air Pressure'] = air_pres_filled.at[hour]
                df_1min.loc[df_1min.index.floor('h') == hour, 'Weather Rel Humidity'] = rel_hum_filled.at[hour]

        # Calculate the number of NaN values after filling
        if missing_weather_columns == False:
            number_of_nan_values_after = df_1hr['Weather Temp Air'].isna().sum() + df_1hr['Weather Abs Air Pressure'].isna().sum() + df_1hr['Weather Rel Humidity'].isna().sum()
            filled_knmi_data = number_of_nan_values_before - number_of_nan_values_after

            # Print the results
            if filled_knmi_data > 0 or hours_knmi_used > 0:
                print(f"Filled {filled_knmi_data} NaN values with KNMI data.")
                print(f"Replaced weather data for {hours_knmi_used} hours with KNMI data.")
                knmi_data_used = True

        df_1min = add_cop_values(df_1min)
        df_1hr = convert_to_1_hour_data(df_1min) # Convert to 1 hour data again, because we have added the COP values
    
        if bDebugStopExecutionHere:
            df_1min.to_excel(cmb(pRV, '1min_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))
            df_1hr.to_excel(cmb(pRV, '1hr_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))
            df.to_excel(cmb(pRV, 'RawData_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))

        if bWriteExcel:
            dictHeaderMapping = hHP.genHeaders(df_1min.columns)
            df_1min_newheaders = create_output_dataframe(df_1min, dictHeaderMapping, is_hourly=False)
            df_1hr_newheaders = create_output_dataframe(df_1hr, dictHeaderMapping, is_hourly=True)
            
            # fpath_1min = save_dataframe_with_dates(df_1min_newheaders, dictHeaderMapping, pRVMeetFolder, prefix='1min ')
            # fpath_1hr = save_dataframe_with_dates(df_1hr_newheaders, dictHeaderMapping, pRVMeetFolder, prefix='1hour ')
            # change_files = [fpath_1min, fpath_1hr]

            # Save the variables as pickle files
            variables = {
                # 'change_files': change_files,
                'df_1hr_newheaders': df_1hr_newheaders,
                'sMeetsetFolder': sMeetsetFolder,
                'df_1min_newheaders': df_1min_newheaders,
                'outliers_count': outliers_count
            }
            save_variables(variables)
            
            # convert_excel_output(pBase, change_files)

            prefix = '1min - RV - '
            df_1min_full, weeks_with_year = process_minute_data(df_1min_newheaders, pRVMeetFolder, prefix=prefix)
            prefix = '1hour - RV - '
            df_hourly_full = process_hour_data(df_1min_full, df_1hr_newheaders, pRVMeetFolder, prefix=prefix)

            if bReportWord:
                copy_output_to_automaticreporting(weeks_with_year)
                create_word_documents(sMeetsetFolder, location, weeks_with_year, knmi_data_used, pWord)
            
            
