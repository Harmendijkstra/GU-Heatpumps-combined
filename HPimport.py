# -*- coding: utf-8 -*-
"""
Created on Wed Jun 19 11:14:24 2024

@author: LENLUI
"""

import os
from os.path import join as cmb
import numpy as np
import pandas as pd
from io import StringIO
from datetime import datetime, timedelta
import time
import pytz
import struct
import headerMappingsHP as hHP
from createOutput import save_dataframe_with_dates, convert_excel_output, create_output_dataframe
from enthalpyCalculations import add_enthalpy_calcualations
import pickle
import shutil
import sys
import fitz  # PyMuPDF

sys.path.append('Automatic excel calculations/')
from automatic_excel_proccssing import create_word_documents


# Specify global variables
bReadData = False
bWriteExcel = True
bDebugStopExecutionHere = False # This is used before to run seperate file for the stress test of the script in between
bReadPickles = False # This is used to read the pickles from the previous run, good for debugging, but not for normal use. It will than use var_names to read the pickles
bRunPreviousWeek = False # This variable needs to be True if the script is runned automatically, to get the previous week data 

# If bReadPickles is True, the following variables will be read from the pickles
# var_names = ['change_files', 'df_1hr_newheaders', 'sMeetsetFolder', 'df_1min_newheaders', 'outliers_count'] # Used to save and read variables from pickles
var_names = ['df_1hr_newheaders', 'sMeetsetFolder', 'df_1min_newheaders', 'outliers_count'] # Used to save and read variables from pickles

# By default, the script will use the first two arguments as the meetset folder and location
# One can use this by running the script from the command line with the meetset folder and location as arguments. For example python HPimport.py 'Meetset1-Deventer' 'Deventer'
if len(sys.argv) > 2:
    sMeetsetFolder = sys.argv[1]
    location = sys.argv[2]
else:
    # Else use the default values, that can be set here manually:
    # sMeetsetFolder = 'StresstestDeventer_08-12jul'
    # sMeetsetFolder = 'Deventer_13-17jul'
    # sMeetsetFolder = 'Meetset2-Nunspeet'
    sMeetsetFolder = 'Meetset1-Deventer'
    location = 'Deventer' # Used for Automatic excel calculations within Word documents

if bRunPreviousWeek:
    time_now = datetime.now()
    weekno = time_now.isocalendar()[1]
    firstweekday = time_now - timedelta(days=time_now.weekday() + 7) # Go back 7 days to the previous week, starting from the Monday
    lastweekday = firstweekday + timedelta(days=6) # Go forward 6 days to the Sunday
    sDateStart = firstweekday.strftime('%Y-%m-%d')
    sDateEnd = lastweekday.strftime('%Y-%m-%d')
else:
    # Below is the option to set the date range manually
    sDateStart = '2024-07-15'
    # sDateEnd = '2025-12-31' #inclusive
    sDateEnd = '2024-07-21'


# Some global constants
minimum_WP_power = 500 # Minimum power in We to consider the WP to be running
spec_heat_water = 4.187  # kJ/kgK
# Some coefficients, that were generated by Bertus for a fitted COP curve:
iii = 8.485073
iis = 0.13324
sii = -3.32107
sis = -0.06643
isi = -0.12466
iss = -0.00335
ssi = 0.062575
sss = 0.002186
max_power_wm1 = 12000 # Max power in warmtepomp 1, in We
max_power_wm2 = 17000 # Max power in warmtepomp 2, in We
maximum_interpolate_nans = 20 # Number of NaNs to interpolate in a row before setting the rest to nan
atmospheric_pressure = 1.023  # Atmospheric pressure in bar
minimum_atmospheric_pressure = 700 # Minimum pressure in mbar, below which the pressure is considered to be invalid and atmospheric_pressure is used
# For all variables there is a min and max values setted in minmax_cols.xlsx, but TgasIn and PgasIn are also set here:
TgasIn_min = -5.0
TgasIn_max = 100.0
PgasIn_min = -2.0
PgasIn_max = 70.0


def save_variables(variables, folder='saveTemporaryPickles'):
    if not os.path.exists(folder):
        os.makedirs(folder)
    
    for var_name, var_value in variables.items():
        with open(os.path.join(folder, f'{var_name}.pkl'), 'wb') as f:
            pickle.dump(var_value, f)

def read_variables(var_names, folder='saveTemporaryPickles'):
    variables = {}
    for var_name in var_names:
        with open(os.path.join(folder, f'{var_name}.pkl'), 'rb') as f:
            variables[var_name] = pickle.load(f)
    return variables

def process_and_save(pBase, sMeetset):
    pMeetset = cmb(pInput, sMeetset)
    output_subdir = cmb(pPickles, sMeetset)
    os.makedirs(output_subdir, exist_ok=True)
    print(f'Processing and saving pickles for set {sMeetset}...')

    # Compare ingested file dates in the form of '240607' with lstDone, skip already ingested files.
    lstDone = sorted([f.split('_')[1][2:10].replace('-','') for f in os.listdir(output_subdir) 
                        if (f.endswith('.bz2') and not '_partial' in f)])
    all_files = sorted([f for f in os.listdir(pMeetset) if (f.endswith('.txt') and 
                    not f.split('_')[-2] in lstDone and not '-test' in f and not '-config' in f)])
    daily_data = []
    current_day = None
    lstTimes = []

    for file in all_files:
        print(f' - Reading file {file}..')
        sData, header_df = read_file(cmb(pMeetset, file))
        dfEmpty = create_multi_index_df(header_df)
        tStart = time.time()
        data = process_datafile(sData, dfEmpty, header_df)
        lstTimes.append(time.time()-tStart)

        if not data.empty:
            data[data.columns[0]] = pd.to_datetime(data[data.columns[0]], format='%y/%m/%d %H:%M:%S')
            file_date = data['Timestamp'].iloc[0].iloc[0].date()  # Assuming 'timestamp' column exists and is datetime type
#EDIT python 3.12: iloc[0][0] no longer valid
            if current_day is None:
                current_day = file_date
            if file_date != current_day:
                # Save the accumulated data for the previous day
                daily_df = pd.concat(daily_data)
                daily_df.reset_index(inplace=True, drop=True)
                daily_date = daily_df['Timestamp'].iloc[0].iloc[0].date()
#EDIT python 3.12: iloc[0][0] no longer valid                
                print(f'Saving daily data for date {daily_date}..')
                daily_df = group_and_combine_columns(daily_df)
                daily_df.to_pickle(cmb(output_subdir, f'{sMeetset}_{daily_date}.bz2'))
                if os.path.exists(cmb(output_subdir, f'{sMeetset}_{daily_date}_partial.bz2')):
                    os.remove(cmb(output_subdir, f'{sMeetset}_{daily_date}_partial.bz2'))
                daily_data = []
                current_day = file_date
                print(f'Average processing_datafile execution time was {round(np.mean(lstTimes),2)} sec')
                lstTimes = []
            daily_data.append(data)
    # Handle the last batch of files that do not make up a whole day yet
    if daily_data:
        print(f'Saving partial data for date {file_date}..')
        daily_df = pd.concat(daily_data)
        daily_df.reset_index(inplace=True, drop=True)
        daily_df = group_and_combine_columns(daily_df)
        daily_df.to_pickle(cmb(output_subdir, f'{sMeetset}_{file_date}_partial.bz2'))

def combine_columns(df, cols, combined_col_name):
    """
    Combine a list of columns in a DataFrame, giving precedence to non-NaN values.

    This function takes a DataFrame and a list of column names, and combines these columns into a single column.
    The resulting column will have non-NaN values from the first column in the list, and if a value is NaN, it will
    be replaced by the corresponding non-NaN value from the next column in the list, and so on. The combined column
    is returned as a DataFrame with the specified column name.

    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to combine.
    cols (list): A list of column names to combine.
    combined_col_name (tuple): The name for the combined column.
    
    Returns:
    pd.Series: A Series with the combined values.
    """
    combined = df[cols[0]]
    for col in cols[1:]:
        combined = combined.combine_first(df[col])
    return pd.DataFrame({combined_col_name: combined})

def group_and_combine_columns(df, combine_column='Signal No.'):
    """
    Group columns by common indices except for the 'Signal No.' level and combine them.
    
    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to group and combine.
    combine_column (str): The multi index column that should be combined, hence ignored if it has different 
    
    Returns:
    pd.DataFrame: The DataFrame with combined columns.
    """
    # Find the index position of combine_column
    signal_no_index = df.columns.names.index(combine_column)

    # Group columns by common indices except for the signal number
    grouped_columns = {}
    for col in df.columns:
        key = tuple([col[i] for i in range(len(col)) if i != signal_no_index])  # Exclude the signal number index
        if key not in grouped_columns:
            grouped_columns[key] = []
        grouped_columns[key].append(col)

    # Create a list to store the combined columns
    combined_columns = []

    # Combine columns in each group
    for key, cols in grouped_columns.items():
        if len(cols) > 1:
            combined_col_name = key[:signal_no_index] + ('Combined',) + key[signal_no_index:]
            combined_col_df = combine_columns(df, cols, combined_col_name)
        else:
            # Adjust the column name to include 'Combined' in the correct position
            single_col_name = key[:signal_no_index] + ('Combined',) + key[signal_no_index:]
            combined_col_df = df[cols].copy()
            combined_col_df.columns = pd.MultiIndex.from_tuples([single_col_name])
        
        combined_columns.append(combined_col_df)

    # Concatenate all combined columns into a new DataFrame
    combined_df = pd.concat(combined_columns, axis=1)

    # Set the names of the MultiIndex levels
    combined_df.columns.names = df.columns.names

    return combined_df

def read_file(fPath):
    """
    Reads the data rows line by line and processes them into a DataFrame.
    Separately reads the first rows as headers.
    """
    # Open the file in read mode
    with open(fPath, 'r') as file:
        # Read the entire contents of the file into a string
        sData = file.read()
    # Get rid of the trailing separator on each row
    sData = sData.replace(";\n","\n")
    
    # Read the header rows
    header_data = StringIO('\n'.join(sData.split('\n')[:4]))
    header_df = pd.read_csv(header_data, sep=';', header=None)
    
    return sData, header_df


def create_multi_index_df(header_df):
    """
    Creates the multi_index Dataframe from the header rows, returns an empty df with multi-index
    """
    # Create the multi-index
    nSignals = len(header_df.iloc[0])
    # multi_index = pd.MultiIndex.from_arrays([header_df.iloc[1], header_df.iloc[2], header_df.iloc[3], header_df.iloc[0]], names=['Signal Name', 'Unit', 'Tag', 'Signal No.'])
    
    # Below is the option for a MultiIndex that stores [Value, status_bit] for each signal
    multi_indexValues = pd.MultiIndex.from_arrays([header_df.iloc[1], header_df.iloc[2], header_df.iloc[3], header_df.iloc[0], ['Value']*nSignals], names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    multi_indexStatus = pd.MultiIndex.from_arrays([header_df.iloc[1, 1:], header_df.iloc[2, 1:], header_df.iloc[3, 1:], header_df.iloc[0, 1:], ['Status']*(nSignals-1)], names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    lst = multi_indexValues.to_flat_index().append(multi_indexStatus.to_flat_index())
    multi_index = pd.MultiIndex.from_tuples(lst, names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    
    # Initialize an empty DataFrame with the multi-index columns
    dfEmpty = pd.DataFrame(columns=multi_index)
    return dfEmpty

def process_datafile(sData, dfEmpty, header_df):
    """
    Read the sData string, and use the multi-index empty dataframe to create 
    the full dataframe for each file, with a multi-index header
    """    
    # Make sure the timestamp column is set up as multi-index as well
    sTimestampCol = dfEmpty.columns[0]
    rows = []
    data_rows = sData.split('\n')[4:]
    # Determine the lowest signal no. to use in indexing
    # lstSignalNos = [0] + [int(x[3]) for x in dfEmpty.columns[1:]]
    lstSignalNos = header_df.iloc[0]
    # nSignalNoOffset = min() - 1
    
    # Process each data row
    for cnt, row in enumerate(data_rows):
        if row.startswith('D'):
            if row.endswith(';'):
                row = row[:-1]
            parts = row.split(';')
            timestamp = parts[1]
            data_dict = {sTimestampCol: timestamp}
            # Skip row if there are not complete sets of 3 values on each row
            if np.mod(len(parts)-2, 3) != 0:
                print(f'data_row[{cnt}] skipped, because amount of data values is not a multiple of 3 [signal_no, value, status]')
                continue
            elif len(parts) < 5:
                print(f'data_row[{cnt}] skipped, no valid data found')
                continue
            for i in range(2, len(parts), 3):
                if i + 2 < len(parts):
                    signal_number = int(parts[i])
                    if parts[i + 1] in ['','T-Mobile  NL','LTE CAT M1','False']:
                        signal_value = np.nan
                    else:
                        signal_value = float(parts[i + 1])
                    status_bit = int(parts[i + 2])
                    # nCol = lstSignalNos.index(signal_number)
                    nCol = lstSignalNos[lstSignalNos == str(signal_number)].index[0]
                    signal_name = header_df.iloc[1, nCol]
                    unit = header_df.iloc[2, nCol]
                    tag = header_df.iloc[3, nCol]
                    # tag = header_df.iloc[3, signal_number - nSignalNoOffset]
                    data_dict[(signal_name, unit, tag, signal_number, 'Value')] = signal_value
                    data_dict[(signal_name, unit, tag, signal_number, 'Status')] = status_bit
            # df = df.append(data_dict, ignore_index=True)
            rows.append(data_dict)
    
    # Create the DataFrame from the list of rows
    df = pd.DataFrame(rows)
    # Set the multi-index columns
    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Signal Name', 'Unit', 'Tag', 'Signal No.', 'Type'])
    return df


def load_data(sMeetset=None, sDateStart='2024-01-01', sDateEnd='2025-12-31'):
    print(f'\nLoading stored data from {sDateStart} -1 to {sDateEnd}..')
    # Loading one day before, because of 1-2 hr timedelta from UTC to local time
    tStart = datetime.strptime(sDateStart, '%Y-%m-%d').date() - timedelta(days=1)
    tEnd = datetime.strptime(sDateEnd, '%Y-%m-%d').date()
    # Only add files for which the YYYY-mm-dd date falls within tStart to tEnd (inclusive)    pickledir = cmb(pPickles,sMeetset)
    pickledir = cmb(pPickles,sMeetset)
    if not os.path.exists(pickledir):
        os.makedirs(pickledir)

    lstPickles = [f for f in os.listdir(pickledir) if (not f.endswith('partial.bz2') and
                  tStart <= datetime.strptime(f.split('_')[-1][0:10], '%Y-%m-%d').date() <= tEnd) or
                  (f.endswith('partial.bz2') and 
                   tStart <= datetime.strptime(f.split('_')[-2][0:10], '%Y-%m-%d').date() <= tEnd)]
    lstPickleData = []
    for pklfile in lstPickles:
        pkldata = pd.read_pickle(cmb(pPickles,sMeetset,pklfile))
        lstPickleData.append(pkldata)
        print(f'  Loaded file {pklfile}..')
    df = pd.concat(lstPickleData)
    df.reset_index(inplace=True, drop=True)
    return df

def flatten_data(df, bStatus=False, ignore_multi_index_differences=False):
    """
    Group columns by common indices except for the 'Signal No.' level and combine them.
    
    Parameters:
    df (pd.DataFrame): The DataFrame containing the columns to group and combine.
    bStatus (bool): Can be set to True. In that cas wil Status and Value be added as seperate columns
    ignore_multi_index_differences (bool): Can be set to True to ignore differences in the multi index. 
    In that case it would be ignored if there are differences in Unit or Tag.
    
    Returns:
    pd.DataFrame: The DataFrame with combined columns.
    """
    print("Flatten data...")
    if ignore_multi_index_differences:
        df = group_and_combine_columns(df, combine_column='Unit')
        df = group_and_combine_columns(df, combine_column='Tag')
    # Drop the 'Type' level from the MultiIndex columns
    flattened_columns = df.columns.droplevel('Type')
    # Remove duplicates from the flattened columns
    unique_columns = flattened_columns.drop_duplicates()
    # Get all unique signal names
    signal_names = df.columns.get_level_values('Signal Name').unique()
    # Check for each signal name whether it appears only once in the unique columns
    for signal_name in signal_names:
        occurrences = unique_columns[unique_columns.get_level_values('Signal Name') == signal_name]
        count = sum(unique_columns.get_level_values('Signal Name') == signal_name)
        if count > 1:
            raise ValueError(f"There are multiple columns for the signal name '{signal_name}': {count}. Occurrences: {occurrences}")

    if bStatus:
        idxFlatHeader = df.columns.get_level_values(0) + ' (' + df.columns.get_level_values(4) + ')'
    else:
        type_no_index = df.columns.names.index('Type')
        df = df.xs('Value', axis=1, level=type_no_index, drop_level=False)
        idxFlatHeader = df.columns.get_level_values(0)
    dfHeaders = df.columns.to_frame(index=False)
    df.columns = idxFlatHeader
    # df.set_index(idxFlatHeader[0], drop=True, inplace=True)
    return df, dfHeaders

def round_to_nearest_15_seconds(timestamp):
    """Round a timestamp to the nearest 15-second interval."""
    return timestamp.round('15s')

def combine_and_sync_rows(df):
    """Process the DataFrame to round 15sec intervals and fill values."""
    print("Combine and sync rows...")
    # Round timestamps to the nearest 15-second interval
    df['Rounded Timestamp'] = df['Timestamp'].apply(round_to_nearest_15_seconds)
    
    # Group by the rounded timestamps and apply forward fill within each group
    grouped = df.groupby('Rounded Timestamp').apply(lambda group: group.ffill().iloc[-1])
    
    # Reset the index to get a DataFrame
    processed_df = grouped.reset_index(drop=True)
    
    # Drop the original 'Timestamp' column and rename 'Rounded Timestamp' to 'Timestamp'
    processed_df = processed_df.drop(columns=['Timestamp'])
    processed_df = processed_df.rename(columns={'Rounded Timestamp': 'Timestamp'})
    
    return processed_df

def add_hours_based_on_dst(df, sDateStart, sDateEnd):
    """
    Add 1 hour or 2 hours to the 'Timestamp' column depending on whether it is Dutch summer time or winter time.
    """
    print("Add Adjusted Timestamp...")
    # Define the Europe/Amsterdam timezone
    amsterdam_tz = pytz.timezone('Europe/Amsterdam')
    
    def adjust_time(timestamp):
        # Localize the timestamp to Europe/Amsterdam timezone
        localized_timestamp = amsterdam_tz.localize(timestamp)
        
        # Check if the timestamp is in DST
        if localized_timestamp.dst() != timedelta(0):
            # Summer time (DST)
            return timestamp + timedelta(hours=2)
        else:
            # Winter time (Standard Time)
            return timestamp + timedelta(hours=1)
    
    # Rename the original 'Timestamp' column
    df = df.rename(columns={'Timestamp': 'Original Timestamp'})
    
    # Apply the adjust_time function to the 'Original Timestamp' column
    df['Adjusted Timestamp'] = df['Original Timestamp'].apply(adjust_time)
    
    # Drop the 'Original Timestamp' column
    df = df.drop(columns=['Original Timestamp'])
    
    # With the adjusted timestamp, remove the leading/trailing data that falls outside of the date window
    tStart = datetime.strptime(sDateStart, '%Y-%m-%d')
    tEnd = datetime.strptime(sDateEnd, '%Y-%m-%d') + timedelta(days=1)
    rowsDateWindow = df['Adjusted Timestamp'].between(tStart,tEnd)
    df = df.loc[df[rowsDateWindow].index[:-1], :]
    
    return df

def make_totalizers_monotonic(df, columns):
    print('\nChecking df for monotonic increasing counters..')
    for col in columns:
        if df[col].is_monotonic_increasing:
            print(f'Totalizer column {col} is monotonic_increasing.')
            continue
        else:
            print(f'Totalizer column {col} is not monotonic_increasing, correcting..')
            lastValidCounter = df.loc[df.index[0], col]
            for nRow, val in enumerate(df[col]): #range(1, len(df)):
                if val < lastValidCounter:
                    df.loc[df.index[nRow], col] = np.nan
                else:
                    lastValidCounter = val
    df = interpolate_columns(df, columns)
    return df

def interpolate_columns(df, columns, nLimit=None):
    """Interpolate the specified columns in the DataFrame."""
    for col in columns:
        if not 'Timestamp' in col:
            df[col] = df[col].interpolate(method='time', limit=nLimit)
    return df

def process_totalizers(df, totalizer_dict):
    """Process the totalizer columns as specified."""
    for new_col, cols in totalizer_dict.items():
        # Set NaN values at the beginning and end of each column to the first and last non-NaN values
        for col in cols:
            first_valid_index = df[col].first_valid_index()
            last_valid_index = df[col].last_valid_index()
            
            if first_valid_index is not None:
                # first_valid_loc = df.index.get_loc(first_valid_index)
                # df[col].iloc[:first_valid_loc] = df[col].iloc[first_valid_loc]
                df.loc[:first_valid_index, col] = df[col].loc[first_valid_index]
            
            if last_valid_index is not None:
                # last_valid_loc = df.index.get_loc(last_valid_index)
                # df[col].iloc[last_valid_loc + 1:] = df[col].iloc[last_valid_loc]
                df.loc[last_valid_index:, col] = df[col].loc[last_valid_index]

        # Sum the columns to create the new totalizer column
        df[new_col] = df[cols].sum(axis=1)
        
        # Create the _actual column
        df[f'{new_col}_actual'] = df[new_col]
        df[f'{new_col}_actual'] = df[f'{new_col}_actual'].ffill()
        
        # Drop summed column
        df = df.drop(columns=new_col)
    
    # Flatten the list of lists and convert to a set to get unique values
    unique_values_totalizer = set(item for sublist in totalizer_dict.values() for item in sublist)

    # Convert the set back to a list if needed
    unique_values_totalizer_list = list(unique_values_totalizer)
    for col in unique_values_totalizer_list:
        # Drop the original columns
        if col in df.columns:
            df = df.drop(columns=col)
        
    return df

def convert_bits(row, columns, unpack_format='<d'):
    """Function to convert non-NaN values in the specified columns into a single column."""  
    int_list = [int(row[col]) for col in columns if not np.isnan(row[col])]
    if unpack_format == '<d':
        if len(int_list) != 4:
            return np.nan  # Ensure we have exactly 4 values to process
    elif unpack_format.endswith('I'):
        if len(int_list) != 2:
            return np.nan  # Ensure we have exactly 2 values to process    

    modbus_update = datetime.strptime('2024-09-03 09:41:00', '%Y-%m-%d %H:%M:%S') #Modbus update from here on
    # Change conversion after modbus update
    if row['Timestamp'] > modbus_update:
        byte_sequence = b''.join(struct.pack('>H', n) for n in int_list)
    else:
        # Pack each 16-bit integer into a 2-byte sequence
        byte_sequence = b''.join(struct.pack('<H', n) for n in int_list)
        
    # Interpret the byte sequence based on the specified unpack format
    if unpack_format == '<d':  # Double-precision float
        value = round(struct.unpack('<d', byte_sequence)[0], 3)
    elif unpack_format.endswith('I'):  # 32-bit unsigned integer
        value = struct.unpack(unpack_format, byte_sequence)[0]
    else:
        raise ValueError(f"Unsupported unpack format: {unpack_format}")
    
    return value

def convert_to_1_minute_data(df, totalizer_dict):
    """Convert 15-second data to 1-minute data."""
    print("Convert to 1 minute data...")    
    # Set 'Adjusted Timestamp' as the index
    df = df.set_index('Adjusted Timestamp')
    
    # Process the totalizer columns
    df = process_totalizers(df, totalizer_dict)
    
    # Identify non-totalizer columns
    non_totalizer_columns = list(set(df.columns) - set(f'{col}_actual' for col in totalizer_dict.keys()))
    totalizer_columns = [f'{col}_actual' for col in totalizer_dict.keys()]
    
    # Resample to 1-minute intervals and calculate the mean for normal columns
    df_1min_non_totalizer = df[non_totalizer_columns].resample('1min').mean()

    # For totalizer columns, take the value at the zero second within that minute
    df_1min_totalizer = df[totalizer_columns].resample('1min').first()

    # Combine the two DataFrames
    df_1min = pd.concat([df_1min_non_totalizer, df_1min_totalizer], axis=1)

    # Calculate the _diff columns using forward fill to handle NaNs
    for col in totalizer_columns:
        diff_col = col.replace('_actual', '_diff')
        df_1min[diff_col] = df_1min[col].ffill().diff().shift(-1).fillna(0)

    # Reset the index to make 'Adjusted Timestamp' a column again
    df_1min = df_1min.reset_index()
    
    return df_1min

def has_nan_streak(series, threshold=5):
    # Identify consecutive NaN streaks
    streak = series.isna().astype(int).groupby(series.notna().cumsum()).sum()
    # Return True if any streak is greater than or equal to the threshold
    return streak.max() >= threshold

def convert_to_1_hour_data(df_1min):
    df_1hr = pd.DataFrame(columns=df_1min.columns)
    
    # Identify column groups
    colsActual = [col for col in df_1min.columns if col.endswith('_actual')] + ['Adjusted Timestamp']
    colsDiff = [col for col in df_1min.columns if col.endswith('_diff')]
    colsMean = list(set(df_1min.columns) - set(colsActual) - set(colsDiff))
    
    # Resample mean columns (check for 5-minute NaN streaks before calculating the mean)
    def custom_mean(x):
        if has_nan_streak(x, 5):  # If 5 or more consecutive NaNs, return NaN for the hour
            return pd.NA
        return x.mean()

    df_1hr[colsMean] = df_1min[colsMean].resample('h').apply(custom_mean)
    
    # Resample actual columns (first value per hour)
    df_1hr[colsActual] = df_1min[colsActual].resample('h').first()
    
    # Resample diff columns (difference between first and last actual values, respecting NaN streaks)
    for col in colsDiff:
        actual_col = col[:-4] + '_actual'  # Strip '_diff' and add '_actual'
        if actual_col in colsActual:
            df_1hr[col] = df_1min[actual_col].resample('h').apply(
                lambda x: (x[-1] - x[0]) if not has_nan_streak(x, 5) else pd.NA
            )
    
    return df_1hr


def interpolate(series, thresh=20):
    # where the nan values are
    nans = series.isna()

    # calculate the size of consecutive `nan`
    mask = nans.groupby([(~nans).cumsum(),nans]).transform('size') > thresh
    return series.interpolate(method='linear', limit_area='inside').mask(mask)

def check_monotonic_and_fill_gaps(df, freq='15s'):
    index = df.index
    
    # Check if the index is monotonic increasing
    if not index.is_monotonic_increasing:
        raise Exception("The DatetimeIndex is not monotonic increasing.")
    
    # Generate the expected range based on the start and end, with the given frequency
    expected_range = pd.date_range(start=index.min(), end=index.max(), freq=freq)
    
    # Reindex the DataFrame to the expected range, filling gaps with NaNs
    df_reindexed = df.reindex(expected_range)
    
    return df_reindexed

def interpolate_nans(df, nLimit=None):
    print('\nInterpolating missing values in df. Current fraction of missing values:')
    dfMissing = pd.DataFrame()
    dfMissing['Sum'] = df.isna().sum()
    dfMissing['Fraction'] = round(df.isna().sum() / df.isna().count(),3)
    print(dfMissing)
    df = df.apply(interpolate, thresh=nLimit)
    # And deal with the first rows as well:
    df.iloc[:, 3:] = df.iloc[:, 3:].bfill(axis=0)
    return df

def calc_weithed_mean_flow(df_1min, col_wm_flow, col_flow, col_tempout, col_tempin):
    df_1min = df_1min.copy()
    required_cols = [col_flow, col_tempout, col_tempin]
    if not all(col in df_1min.columns for col in required_cols):
        print("One or more required columns are missing. Skipping calculation.")
        return df_1min
    temp_diff = df_1min[col_tempout] - df_1min[col_tempin]
    df_1min[col_wm_flow] = (df_1min[col_flow] * spec_heat_water * (temp_diff)).div(3600)
    return df_1min


def sortColumns(df, lstStartCols):
    lstToSort = list(set(df.columns) - set(lstStartCols))
    lstToSort.sort()
    lstNewOrder = lstStartCols + lstToSort
    df = df[lstNewOrder]
    return df

def add_cop_values(df_1min):
    # Create a copy of the input DataFrame to avoid modifying the original data
    df_1min = df_1min.copy()
    # Extract the outdoor air temperature from the DataFrame
    t_omg = df_1min['Weather Temp Air']
    # Extract the internal temperature from the DataFrame
    t_wm = df_1min['Belimo03 Temp2 internal']
    # Calculate the temperature difference between internal and outdoor air
    diff_temp = t_wm - t_omg
    # Calculate the percentage of total power for Eastron01 and Eastron02
    pct_wm1 = df_1min['Eastron01 Total Power'].div(max_power_wm1)
    pct_wm2 = df_1min['Eastron02 Total Power'].div(max_power_wm2)
    # Calculate the coefficient of performance (COP) for wm1
    cop_wm1 = diff_temp * ((isi + t_omg * iss) + pct_wm1 * (ssi + t_omg * sss)) + (iii + t_omg * iss) + pct_wm1 * (ssi + t_omg * sis)
    # Calculate the coefficient of performance (COP) for wm2
    cop_wm2 = diff_temp * ((isi + t_omg * iss) + pct_wm2 * (ssi + t_omg * sss)) + (iii + t_omg * iss) + pct_wm2 * (ssi + t_omg * sis)
    # Set cop_wm1 to NaN if Eastron01 Total Power is below 300 We
    cop_wm1[df_1min['Eastron01 Total Power'] < minimum_WP_power] = np.nan
    # Set cop_wm2 to NaN if Eastron02 Total Power is below 300 We
    cop_wm2[df_1min['Eastron02 Total Power'] < minimum_WP_power] = np.nan
    # Add the calculated COP values to the DataFrame
    df_1min['cop_wm1'] = cop_wm1
    df_1min['cop_wm2'] = cop_wm2
    # Return the modified DataFrame
    return df_1min


def remove_outliers(df, df_minmax, lstHeaderMapping, max_consecutive = 5):
    outliers_count = {}

    for column in df.columns:
        if column in lstHeaderMapping:
            mv_column = lstHeaderMapping[column][0]
            if mv_column in df_minmax.columns:

                # Fetch the min and max values from df_minmax
                try:
                    min_val = df_minmax.loc[2, mv_column]
                    max_val = df_minmax.loc[3, mv_column]
                except KeyError:
                    print(f"Warning: {mv_column} not found in df_minmax. Skipping column {column}.")
                    continue

                # Create mask for outliers
                outliers = (df[column] < min_val) | (df[column] > max_val)

                # Track the number of outliers for this column
                outliers_count[column] = outliers.sum()

                # Group consecutive outliers (reset group numbers when non-outliers appear)
                group = (outliers.diff().ne(0).cumsum()) * outliers
                
                if len(group.unique()) > 1:
                    # Print that this column has outliers
                    print(f"Column '{column}' has {outliers_count[column]} outliers.")

                for grp_num, grp_indices in df[column].groupby(group).groups.items():
                    if grp_num == 0:  # Skip non-outlier groups
                        continue
                    
                    if len(grp_indices) > max_consecutive:  # Set NaN if more than max_consecutive outliers
                        df.loc[grp_indices, column] = np.nan
                    else:  # Replace 1 to max_consecutive outliers with the average of surrounding values
                        first_idx = df.index.get_loc(grp_indices[0])
                        last_idx = df.index.get_loc(grp_indices[-1])
                        
                        # Get the values before and after the sequence based on position
                        before_val = df[column].iloc[first_idx - 1] if first_idx > 0 else np.nan
                        after_val = df[column].iloc[last_idx + 1] if last_idx < len(df) - 1 else np.nan
                        
                        fill_value = np.nanmean([before_val, after_val])
                        df.loc[grp_indices, column] = fill_value

    return df, outliers_count

def copy_output_to_automaticreporting(weeks_with_year):
    src_dir = os.path.join(pRV, sMeetsetFolder)
    dst_dir = os.path.join(pBase, 'Automatic excel calculations', 'Input', sMeetsetFolder)

    for dirpath, dirnames, filenames in os.walk(src_dir):
        print(dirpath)
        rel_path = os.path.relpath(dirpath, src_dir)
        if rel_path != '.':
            dst_path = os.path.join(dst_dir, rel_path)
            if not os.path.exists(dst_path):
                os.makedirs(dst_path)
            else:
                # Remove all files in the destination directory
                for file in os.listdir(dst_path):
                    os.remove(os.path.join(dst_path, file))
            for filename in filenames:
                src_file = os.path.join(dirpath, filename)
                dst_file = os.path.join(dst_path, filename)
                shutil.copy2(src_file, dst_file)

if __name__ == "__main__":
    # Set environment variables
    pBase = os.getcwd()
    pParentDir = os.path.dirname(pBase)
    pInput = cmb(pParentDir,'Collected Data')
    pPickles = cmb(pBase,'ImportedPickles')
    pRaw = cmb(pParentDir,'01. Excel data overview')
    pRV = cmb(pParentDir,'02. Excel MV-RV')
    pWord = cmb(pParentDir,'03. Word results')
    pRVMeetFolder = cmb(pRV, sMeetsetFolder)
    year = sDateStart.split('-')[0]

    
    if bReadPickles: 
        # Note that the function convert_excel_output sometimes results in errors.
        # This happens when a keyboard or mouse click is interrupting the process when excel is opened by the code.
        # Therefore the results are stored and can be read from pickles, but this is overwritten everytime.
        # Thefore make sure you have just runned the code with the correct settings before reading the pickles.
        variables = read_variables(var_names)
        # change_files = variables['change_files']
        df_1hr_newheaders = variables['df_1hr_newheaders']
        sMeetsetFolder = variables['sMeetsetFolder']
        df_1min_newheaders = variables['df_1min_newheaders']
        outliers_count = variables['outliers_count']
        # convert_excel_output(pBase, change_files)
        
        prefix = '1hour - RV - '
        weeks_with_year = add_enthalpy_calcualations(df_1hr_newheaders, pRVMeetFolder, year, prefix=prefix)
        prefix = '1min - RV - '
        weeks_with_year = add_enthalpy_calcualations(df_1min_newheaders, pRVMeetFolder, year, prefix=prefix)
        copy_output_to_automaticreporting(weeks_with_year)
        create_word_documents(sMeetsetFolder, location, weeks_with_year, pWord)
    else:
        

        # Read data into pickles
        if bReadData:
            process_and_save(pBase, sMeetsetFolder)

        dfRaw = load_data(sMeetset=sMeetsetFolder, sDateStart = sDateStart, sDateEnd = sDateEnd)
        df, dfHeaders = flatten_data(dfRaw, bStatus=False, ignore_multi_index_differences=True)
        # raise ValueError("Stopping here for debugging..")

        # BACKUP columns for if Weather Air Temp value is off, 
        # then reconstruct with a separate function from 03A+03B most likely
        weatherAirTempCols = [col for col in df.columns if col.startswith('Weather Temp Air')]
        df.drop(weatherAirTempCols[1:], axis=1, inplace=True)
        if 'Itron Gas volume 2' in df.columns:
            df.drop(['Itron Gas volume 2','Itron Gas volume 3'], axis=1, inplace=True)

        process_stream_1p = ['Stream1 PressureA', 'Stream1 PressureB', 'Stream1 PressureC', 'Stream1 PressureD']
        process_stream_1t = ['Stream1 TemperatureA', 'Stream1 TemperatureB', 'Stream1 TemperatureC', 'Stream1 TemperatureD']
        process_stream_1f = ['Stream1 FlowA', 'Stream1 FlowB', 'Stream1 FlowC', 'Stream1 FlowD']
        process_stream_2p = ['Stream2 PressureA', 'Stream2 PressureB', 'Stream2 PressureC', 'Stream2 PressureD']
        process_stream_2t = ['Stream2 TemperatureA', 'Stream2 TemperatureB', 'Stream2 TemperatureC', 'Stream2 TemperatureD']
        process_stream_2f = ['Stream2 FlowA', 'Stream2 FlowB', 'Stream2 FlowC', 'Stream2 FlowD']
        
        print("Convert bits to values...")
        # Apply the function to each row and create a new column with the results
        dictBitConversionEVHI = {'Stream1 Pressure':process_stream_1p, 'Stream1 Temperature':process_stream_1t, 
                                'Stream1 Flow':process_stream_1f, 'Stream2 Pressure':process_stream_2p, 
                                'Stream2 Temperature':process_stream_2t,'Stream2 Flow':process_stream_2f}
        for key,value in dictBitConversionEVHI.items():
            if value[0] in df.columns:
                df[key] = df.apply(lambda row: convert_bits(row, value, unpack_format='<d'), axis=1)
                df.drop(value, axis=1, inplace=True)

        if bDebugStopExecutionHere: 
            print('Stopping here for debugging')   
            raise ValueError("Stopping here for debugging..")
        else:
            df = combine_and_sync_rows(df)
            df = add_hours_based_on_dst(df, sDateStart, sDateEnd)

        
        # Exctract the keys from header list that are needed for this code to run
        # Remove diff if added, to get the original columns
        columns_keys = list(hHP.makeAllHeaderMappings().keys())
        original_columns = [item.replace('_diff', '') if item.endswith('_diff') else item for item in columns_keys]
        # Some columns are created, ignore these
        ignore_columns = ['Contains missing data', 'Missing data (no Eastron02)', 
                        'Missing data (no Belimo)', 'Eastron Total Power']
        needed_columns = list(set(original_columns)- set(ignore_columns))
        # Check that only existing columns remain
        needed_columns = list(set(needed_columns).intersection(set(df.columns)))
        # Some columns are a lot of times NaN, include those
        ignore_eastron2_column = [col for col in needed_columns if col.startswith('Eastron02')]
        needed_columns_without_eastron2 = list(set(needed_columns)- set(ignore_eastron2_column))
        exclude_belimo_columns = [col for col in needed_columns if col.startswith('Belimo')]
        needed_columns_without_belimo = list(set(needed_columns_without_eastron2)- set(exclude_belimo_columns))
        # df['Contains missing data'] = df[needed_columns].isnull().any(axis=1)
        df['Missing data (no Eastron02)'] = df[needed_columns_without_eastron2].isnull().any(axis=1)
        df['Missing data (no Belimo)'] = df[needed_columns_without_belimo].isnull().any(axis=1)

        df = sortColumns(df, ['Adjusted Timestamp','Missing data (no Eastron02)','Missing data (no Belimo)'])
        # Write to Excel the full dataframe
        print('Saving full dataframe...')
        df.set_index('Adjusted Timestamp', drop=False, inplace=True)
        
        # Save the data before correcting for outliers and interpolating missing values
        filename = 'Before_corrections ' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx') + '.xlsx'
        df = check_monotonic_and_fill_gaps(df, freq='15s')
        filedir = cmb(pRaw, sMeetsetFolder)
        filepath = cmb(pRaw, sMeetsetFolder, filename)
        if not os.path.exists(filedir):
            os.makedirs(filedir)
        df.to_excel(filepath)
        
        df = interpolate_nans(df, nLimit=maximum_interpolate_nans)
        # First interpolate, then make sum of Eastron power. Otherwise, some interpolated values are missing in the sum.
        if 'Weather Abs Air Pressure' in df.columns:
            # Convert pgasin from barg to bara, but if air pressure is below minimum_atmospheric_pressure mbar, add atmospheric_pressure to the value instead of reading the air pressure
            df['PgasIn'] = np.where(
                df['Weather Abs Air Pressure'] < minimum_atmospheric_pressure, atmospheric_pressure + df['PgasIn'],
                df['Weather Abs Air Pressure'].div(1000) + df['PgasIn']
            )
        if 'Eastron01 Total Power' in df.columns and 'Eastron02 Total Power' in df.columns:
            df['Eastron Total Power'] = df[['Eastron01 Total Power', 'Eastron02 Total Power']].sum(axis=1)
            rowsMaskNAN = df[['Eastron01 Total Power', 'Eastron02 Total Power']].isna().all(axis=1)
            df.loc[df[rowsMaskNAN].index, 'Eastron Total Power'] = np.nan

        # Check on outlier values:
        # TgasIn: Outliers for temperature and PgasIn: outliers for incoming pressure
        rowsMask = df[~df['TgasIn'].between(TgasIn_min, TgasIn_max)].index
        df.loc[rowsMask, 'TgasIn'] = np.nan
        rowsMask = df[~df['PgasIn'].between(PgasIn_min, PgasIn_max)].index
        df.loc[rowsMask, 'PgasIn'] = np.nan
        
        # Handle totalizers, make them consistent and then use for 1min output.
        dict_totalizer = {
            'Itron Gas volume 1': ['Itron Gas volume 1'],
            'Belimo01 FlowTotalL': ['Belimo01 FlowTotalL'],
            'Belimo02 FlowTotalL': ['Belimo02 FlowTotalL'],
            'Belimo03 FlowTotalL': ['Belimo03 FlowTotalL'],
            'BelimoValve FlowTotalL': ['BelimoValve FlowTotalL'],
            # Add 'Hager Total Energy' as well? Currently not used, and no errors..
            }
        dict_totalizer = {k: dict_totalizer[k] for k in dict_totalizer if k in df.columns}
        colsTotalizer = [item for sublist in list(dict_totalizer.values()) for item in sublist]
        df = make_totalizers_monotonic(df, colsTotalizer)
        
        # Remove lines where no data was logged, so check a few basic columns are all nan:
        dropIndices = df[df[['Missing data (no Belimo)','PgasIn','TgasIn']].isna().all(axis=1)].index        
        
        df.drop(dropIndices.unique(), axis=0, inplace=True)

        # Now work on the 1min output for the energy balance calculation    
        df_1min = convert_to_1_minute_data(df, dict_totalizer)
        df_1min.set_index('Adjusted Timestamp', drop=False, inplace=True)
        
        if 'Itron Gas volume 1_diff' in df_1min.columns:
            # df_1min['Itron Gas volume 1_diff'] = df_1min['Itron Gas volume 1_diff']*60*1000 # Convert from m3/minute to l/h
            df_1min['Itron Gas volume 1_diff'] = df_1min['Itron Gas volume 1_diff']*1000 # Convert from m3/minute to l/h
        if 'Belimo03 FlowTotalL_diff' in df_1min.columns:
            print('No change for Belimo')
            # df_1min['Belimo01 FlowTotalL_diff'] = df_1min['Belimo01 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['Belimo02 FlowTotalL_diff'] = df_1min['Belimo02 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['Belimo03 FlowTotalL_diff'] = df_1min['Belimo03 FlowTotalL_diff']*60 # Convert from litre/minute to to l/h
            # df_1min['BelimoValve FlowTotalL_diff'] = df_1min['BelimoValve FlowTotalL_diff']*60 # Convert from litre/minute to to l/h

        # DONE here for older data sets: copy flow rates where _diff exists and 'FlowRate' is empty, to FlowRate
        if bDebugStopExecutionHere:
            belimoCorr = ['01','02','03','Valve']
            for sNum in belimoCorr:
                rowsNoFlow =  df_1min[df_1min[f'Belimo{sNum} FlowRate'].isna()].index
                rowsWithData = df_1min[df_1min[[f'Belimo{sNum} Temp1 external',f'Belimo{sNum} Temp2 internal',f'Belimo{sNum} FlowTotalL_diff']].notna().all(axis=1)].index
                rowsMask = rowsNoFlow.intersection(rowsWithData)
                df_1min.loc[rowsMask, f'Belimo{sNum} FlowRate'] = df_1min.loc[rowsMask,f'Belimo{sNum} FlowTotalL_diff']

        # lstBelimoTotals = ['Belimo01 FlowTotalM3', 'Belimo02 FlowTotalM3', 'Belimo03 FlowTotalM3']
        # for col in lstBelimoTotals:
        #     if col in df_1min.columns:
        #         df_1min[col] = df_1min[col]*60*10 # Convert first from m3*100/minute to m3/h and than to l/h
        df_1min = sortColumns(df_1min, ['Adjusted Timestamp','Missing data (no Eastron02)','Missing data (no Belimo)'])
        minmax_filepath = cmb(pBase, "minmax_cols.xlsx")
        df_minmax = pd.read_excel(minmax_filepath)
        df_1min, outliers_count = remove_outliers(df_1min, df_minmax, lstHeaderMapping=hHP.makeAllHeaderMappings())
        
        # Add some columns with weighted mean flow rates
        df_1min = calc_weithed_mean_flow(df_1min, col_wm_flow='Q_ket1_wm', col_flow='Belimo01 FlowRate', col_tempout='Belimo01 Temp2 internal', col_tempin='Belimo01 Temp1 external')
        df_1min = calc_weithed_mean_flow(df_1min, col_wm_flow='Q_OV_wm', col_flow='Belimo02 FlowRate', col_tempout='Belimo02 Temp2 internal', col_tempin='Belimo02 Temp1 external')
        df_1min = calc_weithed_mean_flow(df_1min, col_wm_flow='Q_WP_wm', col_flow='Belimo03 FlowRate', col_tempout='Belimo03 Temp2 internal', col_tempin='Belimo03 Temp1 external')
        df_1min = calc_weithed_mean_flow(df_1min, col_wm_flow='Q_klep_wm', col_flow='BelimoValve FlowRate', col_tempout='BelimoValve Temp2 internal', col_tempin='BelimoValve Temp1 external')

        df_1min = add_cop_values(df_1min)

        df_1hr = convert_to_1_hour_data(df_1min)
        
        df_1hr['Itron Gas volume 1_diff'] = df_1hr['Itron Gas volume 1_diff']*1000 # Convert first from m3/h to l/h']


        if bDebugStopExecutionHere:
            df_1min.to_excel(cmb(pRV, '1min_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))
            df_1hr.to_excel(cmb(pRV, '1hr_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))
            df.to_excel(cmb(pRV, 'RawData_' + sMeetsetFolder + datetime.now().strftime('_%Y-%m-%d_%Hh%M.xlsx')))


        if bWriteExcel:
            dictHeaderMapping = hHP.genHeaders(df_1min.columns)
            df_1min_newheaders = create_output_dataframe(df_1min, dictHeaderMapping, is_hourly=False)
            df_1hr_newheaders = create_output_dataframe(df_1hr, dictHeaderMapping, is_hourly=True)
            
            # fpath_1min = save_dataframe_with_dates(df_1min_newheaders, dictHeaderMapping, pRVMeetFolder, prefix='1min ')
            # fpath_1hr = save_dataframe_with_dates(df_1hr_newheaders, dictHeaderMapping, pRVMeetFolder, prefix='1hour ')
            # change_files = [fpath_1min, fpath_1hr]

            # Save the variables as pickle files
            variables = {
                # 'change_files': change_files,
                'df_1hr_newheaders': df_1hr_newheaders,
                'sMeetsetFolder': sMeetsetFolder,
                'df_1min_newheaders': df_1min_newheaders,
                'outliers_count': outliers_count
            }
            save_variables(variables)
            
            # convert_excel_output(pBase, change_files)
            
            prefix = '1hour - RV - '
            weeks_with_year = add_enthalpy_calcualations(df_1hr_newheaders, pRVMeetFolder, year, prefix=prefix)
            prefix = '1min - RV - '
            weeks_with_year = add_enthalpy_calcualations(df_1min_newheaders, pRVMeetFolder, year, prefix=prefix)
            copy_output_to_automaticreporting(weeks_with_year)
            create_word_documents(sMeetsetFolder, location, weeks_with_year, pWord)
            
            
            